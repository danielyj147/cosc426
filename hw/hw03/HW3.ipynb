{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd91e7f3-3db3-465f-8437-5c604d3f2494",
   "metadata": {},
   "source": [
    "## HW 3: Evaluating language models \n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to run your experiments for Part 1, load and display from your experiment from Part 2, and answer the questions in all parts. Feel free to add as many code and markdown chunks as you would like in each of the sub-sections. \n",
    "\n",
    "**If you use any external resources (e.g., code snippets, reference articles), please cite them in comments or text!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96443a87-57c4-4c18-a825-9e9aa8cd0f32",
   "metadata": {},
   "source": [
    "## Part 1: Train and evaluate bigram and trigram models \n",
    "\n",
    "In this section, include the experiments to train and evaluate bigram and trigram models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc72e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for reloading HW3.py automatically when it changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9358f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Cache not found at data/alice_in_wonderland_tokens.pkl. Running preprocess...\n",
      "Reading data/alice_in_wonderland.txt\n",
      "Tokenized 1 lines\n",
      "Tokenized 101 lines\n",
      "Tokenized 201 lines\n",
      "Tokenized 301 lines\n",
      "Tokenized 401 lines\n",
      "Tokenized 501 lines\n",
      "Tokenized 601 lines\n",
      "Tokenized 701 lines\n",
      "Tokenized 801 lines\n",
      "Tokenized 901 lines\n",
      "Tokenized 1001 lines\n",
      "Tokenized 1101 lines\n",
      "Tokenized 1201 lines\n",
      "Tokenized 1301 lines\n",
      "Tokenized 1401 lines\n",
      "Tokenized 1501 lines\n",
      "Tokenized 1601 lines\n",
      "Tokenized 1701 lines\n",
      "Tokenized 1801 lines\n",
      "Tokenized 1901 lines\n",
      "Tokenized 2001 lines\n",
      "Tokenized 2101 lines\n",
      "Tokenized 2201 lines\n",
      "Tokenized 2301 lines\n",
      "Tokenized 2401 lines\n",
      "Tokenized 2501 lines\n",
      "Tokenized 2601 lines\n",
      "Tokenized 2701 lines\n",
      "Tokenized 2801 lines\n",
      "Tokenized 2901 lines\n",
      "Tokenized 3001 lines\n",
      "Tokenized 3101 lines\n",
      "Tokenized 3201 lines\n",
      "Tokenized 3301 lines\n",
      "Tokenized 3401 lines\n",
      "Tokenized 3501 lines\n",
      "Tokenized 3601 lines\n",
      "Tokenized 3701 lines\n",
      "Saving preprocessed tokens to cache: data/alice_in_wonderland_tokens.pkl\n",
      "Successfully processed data/alice_in_wonderland.txt. Token count: 49674\n",
      "\n",
      "Cache not found at data/through_the_looking_glass_tokens.pkl. Running preprocess...\n",
      "Reading data/through_the_looking_glass.txt\n",
      "Tokenized 1 lines\n",
      "Tokenized 101 lines\n",
      "Tokenized 201 lines\n",
      "Tokenized 301 lines\n",
      "Tokenized 401 lines\n",
      "Tokenized 501 lines\n",
      "Tokenized 601 lines\n",
      "Tokenized 701 lines\n",
      "Tokenized 801 lines\n",
      "Tokenized 901 lines\n",
      "Tokenized 1001 lines\n",
      "Tokenized 1101 lines\n",
      "Tokenized 1201 lines\n",
      "Tokenized 1301 lines\n",
      "Tokenized 1401 lines\n",
      "Tokenized 1501 lines\n",
      "Tokenized 1601 lines\n",
      "Tokenized 1701 lines\n",
      "Tokenized 1801 lines\n",
      "Tokenized 1901 lines\n",
      "Tokenized 2001 lines\n",
      "Tokenized 2101 lines\n",
      "Tokenized 2201 lines\n",
      "Tokenized 2301 lines\n",
      "Tokenized 2401 lines\n",
      "Tokenized 2501 lines\n",
      "Tokenized 2601 lines\n",
      "Tokenized 2701 lines\n",
      "Tokenized 2801 lines\n",
      "Tokenized 2901 lines\n",
      "Tokenized 3001 lines\n",
      "Tokenized 3101 lines\n",
      "Tokenized 3201 lines\n",
      "Tokenized 3301 lines\n",
      "Tokenized 3401 lines\n",
      "Tokenized 3501 lines\n",
      "Tokenized 3601 lines\n",
      "Tokenized 3701 lines\n",
      "Tokenized 3801 lines\n",
      "Tokenized 3901 lines\n",
      "Tokenized 4001 lines\n",
      "Tokenized 4101 lines\n",
      "Tokenized 4201 lines\n",
      "Tokenized 4301 lines\n",
      "Tokenized 4401 lines\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import HW3 as h\n",
    "\n",
    "# The new, clean way to preprocess and cache your files\n",
    "aiwl_model = h.preprocess_and_cache(\n",
    "    textfname=\"data/alice_in_wonderland.txt\", \n",
    "    lower=True, \n",
    "    tokenizer=h.hf_tokenize, \n",
    "    modelname=\"distilgpt2\"\n",
    ")\n",
    "\n",
    "ttlg_model = h.preprocess_and_cache(\n",
    "    textfname=\"data/through_the_looking_glass.txt\", \n",
    "    lower=True, \n",
    "    tokenizer=h.hf_tokenize, \n",
    "    modelname=\"distilgpt2\"\n",
    ")\n",
    "\n",
    "sh_model = h.preprocess_and_cache(\n",
    "    textfname=\"data/sherlock_holmes.txt\", \n",
    "    lower=True, \n",
    "    tokenizer=h.hf_tokenize, \n",
    "    modelname=\"distilgpt2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HW3 module is not an IPython extension.\n",
      "Using smoothing with k=0.001\n",
      "Using smoothing with k=0.001\n"
     ]
    }
   ],
   "source": [
    "vocab = h.get_hf_vocab('distilgpt2')\n",
    "bigram_model = h.train_ngram(aiwl_model, n=2, smooth=\"add-0.001\", vocab=vocab)\n",
    "trigram_model = h.train_ngram(aiwl_model, n=3, smooth=\"add-0.001\", vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eab7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fd48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd4645",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m h.evaluate(bigram_model, \u001b[43mmodel\u001b[49m, n=\u001b[32m2\u001b[39m, vocab=vocab)\n\u001b[32m      2\u001b[39m h.evaluate(trigram_model, model, n=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "h.evaluate(bigram_model, ttlg_model, n=2, vocab=vocab)\n",
    "h.evaluate(trigram_model, ttlg_model, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259f82b-a926-43f3-9692-767130347103",
   "metadata": {},
   "source": [
    "## Part 2: Train and evaluate `distilgpt2` model\n",
    "\n",
    "In this part, load in the results from your trained `distilgpt2` model from NLPScholar, and evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f6506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3e5180-baba-4983-9614-8699a17b2deb",
   "metadata": {},
   "source": [
    "## Part 3: Reflect on the role of tokenizer\n",
    "In this part, answer the question in `HW3.md` in markdown chunks. If you used external sources to find and make sense of this, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524849f-ba17-450e-88d6-2927644d0130",
   "metadata": {},
   "source": [
    "## Part 4 (Optional): Explore the effect of tokenizer and vocab on ngram model performance\n",
    "\n",
    "In this part, include the results from your experiments (if you choose to attempt this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
