{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd94d006-85a8-4c99-86dc-75f81f2b5b88",
   "metadata": {},
   "source": [
    "# HW 4: Text Classification\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer questions and load + display from your experiments. Feel free to add as many code and markdown chunks as you would like in each of the sub-sections. \n",
    "\n",
    "**If you use any external resources (e.g., code snippets, reference articles), please cite them in comments or text!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b9e1d-646a-49c2-8e2e-62b6c194d830",
   "metadata": {},
   "source": [
    "## Part 1: Train and evaluate `distilgpt` based Bayesian Classifier\n",
    "\n",
    "In this part, load in the probability estimates from your finetuned **language models**, use it to classify text, and display classification results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bd4fb",
   "metadata": {},
   "source": [
    "We want to calculate the probability of text given its class(= likelihood), so we want to finetune two separate models, one trained on only the positive sentiment text, and the other trained on only the negative sentiment text. \n",
    "\n",
    "For the train configuration, we will use minimal pair comparison mode with `loadPretrained` set to `true`(or not specified since it is true by default) since we want to finetune the distilgpt model. \n",
    "\n",
    "Also, since we want to train 2 separate models, we will have 2 diffrent pairs of train-val datasets one with positive reviews and another with negative reviews. \n",
    "\n",
    "The preprocessed training/validation data will be in the format of \n",
    "\n",
    "| text |\n",
    "| :-------: |\n",
    "| `review` |\n",
    "| ... |\n",
    "\n",
    "In order to calculate the accuracy of the model, we need to compare the likelihoods of neg & pos then use the one with higehr log probability as `predicted` to calculate the accuracy. \n",
    "\n",
    "| sentid | pairid | comparison | sentence |\n",
    "| :-------: | :------: | :-------: | :-------: | \n",
    "| 0 | 0 | expected | `review` |\n",
    "| 1 | 1 | expected | `review` |\n",
    "| ... | ... | ... | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fae9b",
   "metadata": {},
   "source": [
    "### Train & Val Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c9e19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "nf = \"./aclImdb/train/neg\"\n",
    "ntfs = [f\"{nf}/{path}\" for path in os.listdir(nf)[:2500]]  # negative train files\n",
    "nvfs = [f\"{nf}/{path}\" for path in os.listdir(nf)[2500:3000]]  # negative val files\n",
    "\n",
    "pf = \"./aclImdb/train/pos\"\n",
    "ptfs = [f\"{pf}/{path}\" for path in os.listdir(pf)[:2500]]  # positive train files\n",
    "pvfs = [f\"{pf}/{path}\" for path in os.listdir(pf)[2500:3000]]  # positive val files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c103896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def txts2tsv(paths: list[str], savefpath: str) -> None:\n",
    "    \"\"\"Generates a tsv file consisted of the frist line of each file\n",
    "\n",
    "    Args:\n",
    "        paths (list[str]): paths of source files\n",
    "        savefpath (str): save path\n",
    "    \"\"\"\n",
    "    rows = [['text']]\n",
    "    for path in paths:\n",
    "        with open(path, 'r') as f:\n",
    "            rows.append([f.readline()])\n",
    "\n",
    "    with open(savefpath, \"w\", newline=\"\") as f:\n",
    "        fw = csv.writer(f)\n",
    "        fw.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86e0fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {\n",
    "    'data/neg_train.tsv': ntfs,\n",
    "    'data/neg_val.tsv': nvfs,\n",
    "    'data/pos_train.tsv': ptfs,\n",
    "    'data/pos_val.tsv': pvfs\n",
    "    }\n",
    "\n",
    "for savefpath, paths in t.items():\n",
    "    txts2tsv(paths, savefpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7e7af",
   "metadata": {},
   "source": [
    "### Eval Dataset Genration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6800ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nef = \"./aclImdb/test/neg\"\n",
    "nefs = [f\"{nef}/{path}\" for path in os.listdir(nef)[:1000]]  # negative eval files\n",
    "\n",
    "pef = \"./aclImdb/test/pos\"\n",
    "pefs = [f\"{pef}/{path}\" for path in os.listdir(pef)[:1000]]  # positive eval files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "350e14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = {\n",
    "    'neg': nefs,\n",
    "    'pos': pefs\n",
    "    }\n",
    "sentid = 0\n",
    "pairid = 0\n",
    "rows = [['sentid', 'pairid', 'comparison', 'sentence', 'target']]\n",
    "for target, paths in e.items():\n",
    "    for path in paths:    \n",
    "        with open(path, 'r') as f:\n",
    "            rows.append([sentid, pairid, \"expected\", f.readline(), target])\n",
    "            sentid += 1\n",
    "            pairid += 1\n",
    "\n",
    "with open('data/bay_eval.tsv', 'w') as f:\n",
    "    fw = csv.writer(f, delimiter='\\t')\n",
    "    fw.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f83bbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ndf = pd.read_csv('predictions/neg_predictions.tsv', sep='\\t')\n",
    "pdf = pd.read_csv('predictions/pos_predictions.tsv', sep='\\t')\n",
    "df = pd.concat([ndf, pdf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "814f4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Overall Likeliehood: 0.208\n",
      "Assumed Prior: 0.5\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 40)\n",
    "likelihood = df['prob'].mean()\n",
    "print(f\"Overall Likeliehood: {likelihood:.3f}\")\n",
    "\n",
    "prior = 1/len(['pos', 'neg'])\n",
    "print(f\"Assumed Prior: {prior:.1f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76684489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ndf['logprob'] = np.log(ndf['prob'])\n",
    "pdf['logprob'] = np.log(pdf['prob'])\n",
    "\n",
    "gndf = ndf.groupby([\"sentid\"])[\"logprob\"].mean()\n",
    "gpdf = pdf.groupby([\"sentid\"])[\"logprob\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9808ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = pd.read_csv('data/bay_eval.tsv', sep='\\t')\n",
    "cdf = pd.DataFrame({\"neg_prob\": gndf, \"pos_prob\": gpdf, \"target\": bdf[\"target\"], \"sentence\": bdf[\"sentence\"]})\n",
    "cdf[\"predicted\"] = np.where(\n",
    "    cdf[\"neg_prob\"] > cdf[\"pos_prob\"], \"neg\", \"pos\"\n",
    ")\n",
    "cdf['correct'] = cdf[\"predicted\"] == cdf[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c35e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Accuracy: 86.95 %\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "acc = cdf[\"correct\"].mean()\n",
    "\n",
    "print('=' * 40)\n",
    "print(f'Accuracy: {acc * 100} %')\n",
    "print('=' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f3c6e",
   "metadata": {},
   "source": [
    "In general, P(class∣text)∝P(text∣class) * P(class). However, since we assume a uniform prior, this term is constant and does not affect the final ranking, making the likelihood the sole determining factor. A token likelihood of 0.21 is strong, considering that a model guessing randomly would only have a probability of 1/vocab-size of being correct, a value that is typically very small.\n",
    "To calculate the accuracy, I first evaluated both the `neg_bay` and `pos_bay` models on the `bay_eval.tsv` dataset. Since the model's predictions are generated on a per-token basis, I grouped the tokens by sentence and calculated the average log probability. I chose to use log probabilities instead of raw probabilities because this method prevents numerical underflow and penalizes low-probability tokens more harshly (as log(p)=>-inf as p=>0). I also took the average, rather than the sum, of the log probabilities because averaging normalizes the scores and mitigates the bias caused by different sentence lengths.\n",
    "\n",
    "Next, I compared the likelihoods of the negative and positive sentiments for each sentence and set whichever was higher as the predicted value. The final step was to evaluate whether the predicted value matched the target (or gold) label of the sentence. I then took the mean of these comparisons to calculate the overall accuracy.\n",
    "I was very pleased with the resulting accuracy of 86.95%, which is a significant improvement over the 50% baseline of a random guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64288a30-d61e-4286-a12f-3ed8a5d50733",
   "metadata": {},
   "source": [
    "## Part 2: Train and evaluate a `distilgpt` based TextClassification model\n",
    "In this part, load in and display the results from your finetuned **TextClassification models**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24723c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {\n",
    "    \"data/textclassification_train.tsv\": {'0': ntfs, '1': ptfs},\n",
    "    \"data/textclassification_val.tsv\": {'0': nvfs, '1': pvfs},\n",
    "}\n",
    "\n",
    "for savefpath, sentiments in t.items():\n",
    "    rows = [[\"text\", \"label\"]]\n",
    "    for sentiment, paths in sentiments.items():\n",
    "        for path in paths:\n",
    "            with open(path, \"r\") as f:\n",
    "                text = f.readline()\n",
    "                rows.append([text, sentiment])\n",
    "    with open(savefpath, \"w\", newline=\"\") as f:\n",
    "        fw = csv.writer(f, delimiter='\\t')\n",
    "        fw.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b2d4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = {\n",
    "    \"data/textclassification_eval.tsv\":{\n",
    "    'neg': nefs,\n",
    "    'pos': pefs\n",
    "    }\n",
    "}\n",
    "textid = 0\n",
    "for savefpath, sentiments in e.items():\n",
    "    rows = [[\"textid\", \"text\", \"target\"]]\n",
    "    for sentiment, paths in sentiments.items():\n",
    "        for path in paths:\n",
    "            with open(path, \"r\") as f:\n",
    "                text = f.readline()\n",
    "                rows.append([textid, text, sentiment])\n",
    "                textid += 1\n",
    "    with open(savefpath, \"w\", newline=\"\") as f:\n",
    "        fw = csv.writer(f, delimiter=\"\\t\")\n",
    "        fw.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40f9c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained w/ CPU\n",
    "tdf = pd.read_csv('results/textclassification.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "# Trained w/ CUDA w/ max_length=512\n",
    "ctdf = pd.read_csv(\"results/cuda_textclassification.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "# After discovering `device: cuda` config, I tried training the model on Turing with CUDA\n",
    "# However I was faced with some kind of index mismatch error.\n",
    "\n",
    "# I got curious and was able to solve this error by adding `max_length=512` param to `NLPScholar/src/trainers/HFTextClassificationTrainer.py:52`\n",
    "\n",
    "# Strangley `device: cpu/mps` seems to handle this error, only warning users(\"Sequence length: 1275 is larger than maximum sequence length allowed by the model: 1024.  Using a stride of 512 in calculating token predictabilities.\")\n",
    "\n",
    "# I wanted to see if there would be any difference in the end.\n",
    "\n",
    "# Just as a fun fact, it took me 34m 55s 329ms to train the model with CPU,\n",
    "# but only 1 minuite or so with CUDA. Very glad to have discovered this option.\n",
    "\n",
    "# & Now I sort of understand the industry's obsession with GPUs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d5f3530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>micro-precision</th>\n",
       "      <th>micro-recall</th>\n",
       "      <th>micro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/jeong/Projects/cosc426-projects/cosc426...</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.91451</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.914499</td>\n",
       "      <td>0.9145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  micro-precision  \\\n",
       "0  /Users/jeong/Projects/cosc426-projects/cosc426...           0.9145   \n",
       "\n",
       "   micro-recall  micro-f1  macro-precision  macro-recall  macro-f1  accuracy  \n",
       "0        0.9145    0.9145          0.91451        0.9145  0.914499    0.9145  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>micro-precision</th>\n",
       "      <th>micro-recall</th>\n",
       "      <th>micro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/jeong/Projects/cosc426-projects/cosc426...</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.91457</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.914496</td>\n",
       "      <td>0.9145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  micro-precision  \\\n",
       "0  /Users/jeong/Projects/cosc426-projects/cosc426...           0.9145   \n",
       "\n",
       "   micro-recall  micro-f1  macro-precision  macro-recall  macro-f1  accuracy  \n",
       "0        0.9145    0.9145          0.91457        0.9145  0.914496    0.9145  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tdf)\n",
    "display(ctdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fd8a7",
   "metadata": {},
   "source": [
    "The final results were straightforward. My text classification model achieved 91.45% accuracy, while the Bayesian model scored 86.95%. This performance gap of about 4.5% really just comes down to how each model reads a sentence. The Bayesian classifier is fast and effective due to its simplicity. It essentially just counts words linked to each sentiment. The problem is that this method completely ignores word order, which is a major weakness when it sees sentences with negation or tricky phrasing. The second model's higher accuracy shows it learned to pay attention to context and structure. That 4.5% improvement is really just the value of using a model that can understand how words actually work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafec7b6-883b-47bd-8e51-2fc6ad8453fe",
   "metadata": {},
   "source": [
    "## Part 3: Reflect on the two approaches to classification\n",
    "\n",
    "In this part, answer the question in `HW4.md` in markdown chunks. If you used external sources to find and make sense of this, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f19c6b",
   "metadata": {},
   "source": [
    "The biggest difference is really about knowledge and how it's used. The Bayesian classifier was built from scratch on our dataset. It learned to associate sentiments by simply counting word frequencies, completely ignoring grammar and context.\n",
    "\n",
    "For the `distilgpt` approach, we didn't start from zero. We took a massive, pretrained model that already had a deep understanding of how language works, and just finetuned its existing knowledge for our specific task. So, it's basically the difference between statistical counting versus adapting an already \"intelligent\" system that has captured some meaning from text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888814d-3dd3-4cf1-b109-c7b2676b7691",
   "metadata": {},
   "source": [
    "## Part 4 (Optional): Error analysis comparing the two approaches to classification\n",
    "\n",
    "In this part, include the results from your experiments (if you choose to attempt this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b8634",
   "metadata": {},
   "source": [
    "### Error analysis on the bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6af8a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bayesian\n",
    "acc\n",
    "\n",
    "neg_pre = (cdf.loc[cdf['predicted'] == 'neg', 'correct']).mean()\n",
    "pos_pre = (cdf.loc[cdf['predicted'] == 'pos', 'correct']).mean()\n",
    "\n",
    "neg_rec = (cdf.loc[cdf[\"target\"] == \"neg\", \"correct\"]).mean()\n",
    "pos_rec = (cdf.loc[cdf[\"target\"] == \"pos\", \"correct\"]).mean()\n",
    "\n",
    "neg_f1 = 2 * (neg_pre * neg_rec) / (neg_pre + neg_rec)\n",
    "pos_f1 = 2 * (pos_pre * pos_rec) / (pos_pre + pos_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634503df",
   "metadata": {},
   "source": [
    "### Error analysis on textclassfication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b63c4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TextClassification\n",
    "tcdf = pd.read_csv(\"predictions/textclassification.tsv\", sep=\"\\t\", index_col=0)\n",
    "tcdf[\"correct\"] = tcdf[\"predicted\"] == tcdf[\"target\"]\n",
    "\n",
    "tacc = tdf['accuracy'].iloc[0]\n",
    "\n",
    "tneg_pre = (tcdf.loc[tcdf[\"predicted\"] == \"neg\", \"correct\"]).mean()\n",
    "tpos_pre = (tcdf.loc[tcdf[\"predicted\"] == \"pos\", \"correct\"]).mean()\n",
    "\n",
    "tneg_rec = (tcdf.loc[tcdf[\"target\"] == \"neg\", \"correct\"]).mean()\n",
    "tpos_rec = (tcdf.loc[tcdf[\"target\"] == \"pos\", \"correct\"]).mean()\n",
    "\n",
    "tneg_f1 = 2 * (neg_pre * neg_rec) / (neg_pre + neg_rec)\n",
    "tpos_f1 = 2 * (pos_pre * pos_rec) / (pos_pre + pos_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ae3c4",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6e6ad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "\u001b[92mBayesian classifier\u001b[0m\n",
      "Accuracy| 86.95%\n",
      "Negative| Precision: 0.8549, Recall: 0.8900, F1: 0.8721\n",
      "Positive| Precision: 0.8853, Recall: 0.8490, F1: 0.8668\n",
      "======================================================================\n",
      "\u001b[94mTextClassification model\u001b[0m\n",
      "Accuracy| 91.45%\n",
      "Negative| Precision: 0.9166, Recall: 0.9120, F1: 0.8721\n",
      "Positive| Precision: 0.9124, Recall: 0.9170, F1: 0.8668\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "GREEN = \"\\033[92m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "END = \"\\033[0m\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{GREEN}Bayesian classifier{END}\")\n",
    "print(f\"Accuracy| {acc*100:.2f}%\")\n",
    "print(f\"Negative| Precision: {neg_pre:.4f}, Recall: {neg_rec:.4f}, F1: {neg_f1:.4f}\")\n",
    "print(f\"Positive| Precision: {pos_pre:.4f}, Recall: {pos_rec:.4f}, F1: {pos_f1:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"{BLUE}TextClassification model{END}\")\n",
    "print(f\"Accuracy| {tacc*100:.2f}%\")\n",
    "print(f\"Negative| Precision: {tneg_pre:.4f}, Recall: {tneg_rec:.4f}, F1: {tneg_f1:.4f}\")\n",
    "print(f\"Positive| Precision: {tpos_pre:.4f}, Recall: {tpos_rec:.4f}, F1: {tpos_f1:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc140c",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c40170",
   "metadata": {},
   "source": [
    "The models likely agree on most of the simple sentences, which is why the Bayesian model's accuracy is already high at 87%. The 4.5% accuracy gap comes from how they each handle the tougher cases.\n",
    "\n",
    "The metrics show they make different kinds of mistakes. The Bayesian model is biased. Its higher recall for negative sentences, combined with lower precision, means it tends to mislabel positive sentences as negative. The TextClassification model is not just more accurate. It's more balanced. Its precision and recall scores are nearly the same for both classes, showing it doesn't have a default guess. Its errors are probably on sentences that are truly ambiguous, while the Bayesian model's errors come from its simpler token-wise guessing method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89a452",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
