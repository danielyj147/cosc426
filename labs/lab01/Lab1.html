<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Lab1</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="cosc-426-f24-lab-1">COSC 426 F24 Lab 1</h1>
<h2 id="introduction">Introduction</h2>
<p>The purpose of this lab is to introduce you to the NLPScholar toolkit
we will be using in the class and to serve as a Python refresher. By
completing this lab, you will demonstrate that you can:</p>
<ul>
<li>Describe the structure of the toolkit</li>
<li>Write config files to run different types of experiments</li>
<li>Work with File/IO, lists, dictionaries and strings in Python.</li>
<li>Use the toolkit in the interact mode to develop intuitions about
word probabilities.</li>
</ul>
<h4 id="pre-requisites">Pre-requisites</h4>
<p>This lab assumes that you have already cloned the NLPScholar
repository and have installed the <code>nlp</code> environment by
following the instructions in <code>Install.md</code>.</p>
<h2 id="structure">Structure</h2>
<p>This lab has three parts:</p>
<ol type="1">
<li>Read through the documentation of the toolkit and answer
questions.</li>
<li>Write three helper functions in Python.</li>
<li>Develop intutions about the predictability estimates that the
toolkit returns. To do this, you will select some sentences to explore
with the helper functions from part 2, and answer some questions.</li>
</ol>
<h2 id="provided-files">Provided files</h2>
<ul>
<li>Lab1.py</li>
<li>through-the-looking-glass.txt</li>
<li><a
href="https://docs.google.com/document/d/1uDlc3_U43MaP5jGXF5kHIwanKwY3CHMsAiBMdhAum0Q/edit?usp=sharing">A
google doc template</a> to write responses</li>
</ul>
<h2 id="what-to-submit">What to submit</h2>
<ul>
<li>Lab1.py</li>
<li>A <strong>pdf</strong> of the google doc template with the
answers.</li>
</ul>
<h2 id="part-0">Part 0</h2>
<p>Before starting each lab, get the latest version of the NLPScholar
rep by first navigating to the folder on terminal and then executing</p>
<pre><code>    git pull</code></pre>
<h2 id="part-1-suggested-time-20-minutes">Part 1 (Suggested time: 20
minutes)</h2>
<p>Read through the README of the Toolkit. Use the google doc template
to answer the following questions:</p>
<ol type="1">
<li><p>Which experiment and mode would you use if you want to:</p>
<ul>
<li><p>Train a model classify whether a given sentence is talking about
the election, which experiment and mode would you use?</p></li>
<li><p>Identify the sentiment of each of the words in a dataset of movie
reviews given a model that is already trained on this task.</p></li>
<li><p>Find the word by word probability of an interesting sentence you
found on the internet.</p></li>
<li><p>Find the average accuracy of an existing part-of-speech
tagger.</p></li>
</ul></li>
<li><p>Write a config file that can train a <code>roberta-large</code>
model on the <code>wikipedia</code> <a
href="https://huggingface.co/datasets/wikimedia/wikipedia">dataset</a>,
which is within a larger <code>wikimedia</code> dataset. Set the
modelfpath to <code>wiki_model</code>.</p></li>
<li><p>Where will the model that you trained in step 2 be
saved?</p></li>
<li><p>Youâ€™ve trained a model to detect sarcasm and called it
<code>sarcasm_model</code>. Evaluate this model on a new set of
sentences called <code>test.tsv</code> which is stored in the following
folder: <code>data/sarcasm/</code>. Set the predfpath as
<code>test_results.tsv</code></p></li>
<li><p>Where will the predictions you generated be saved?</p></li>
<li><p>Write a config file that will use the pretrained
<code>huggingartists/taylor-swift</code> causal langauge model, and give
you word by word predictability estimates for any sentence that you
enter.</p></li>
</ol>
<h2 id="part-2-suggested-time-60-minutes">Part 2 (suggested time: 60
minutes)</h2>
<p>Complete the three functions in Lab1.py. Make sure to read the
function headers and docstrings carefully.</p>
<h2 id="part-3-suggested-time-15-minutes">Part 3 (suggested time: 15
minutes)</h2>
<p>Use the code you wrote in Part 2 and the google doc template to
answer the following questions:</p>
<ol type="1">
<li><p>If you ignore all words that occur less than three times, what is
the most frequent word in the <code>through-the-looking-glass.txt</code>
file? What is the least frequent word?</p></li>
<li><p>For the most and least frequent words in your previous answer,
what are the first three sentences in which these words occur? What is
the probability of these words in these three sentences according to the
masked language model <a
href="https://huggingface.co/distilbert/distilbert-base-uncased">distilbert-base-uncased</a>?
Do these probablities make sense? Why or why not?</p></li>
<li><p>If you repeated step 2 with the 500th most and least frequent
words, do any of your observations change?</p></li>
</ol>
</body>
</html>
