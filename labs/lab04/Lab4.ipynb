{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f397c0dc-1fed-4f5b-89a8-e0af21e06c0a",
   "metadata": {},
   "source": [
    "# Lab 3: Grammar writing\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab4.md`. Make sure to include in this notebook all the tests you run to ensure that at each stage your n-gram model implementation is correct. Please use **markdown** chunks to answer any non-code questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda0e4a-6302-4d4e-8c55-cc75a8648a0c",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Answer questions for each of the subparts here. Add as many markdown chunks as needed under each sub-part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef565601-0982-4932-a12d-290e1dd28b60",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83231c5d",
   "metadata": {},
   "source": [
    "text = ['the panda eats the sandwich in her pajamas',\n",
    "         'a sandwich likes a panda',\n",
    "         'the sandwich likes her panda in some pajamas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172b06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 3, 'panda': 3, 'eats': 1, 'sandwich': 3, 'in': 2, 'her': 2, 'pajamas': 2, 'a': 2, 'likes': 2, 'some': 1}\n"
     ]
    }
   ],
   "source": [
    "text = ['the panda eats the sandwich in her pajamas',\n",
    "         'a sandwich likes a panda',\n",
    "         'the sandwich likes her panda in some pajamas']\n",
    "# find the wordcount for each word in the text\n",
    "wordcount = {}\n",
    "for line in text:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word in wordcount:\n",
    "            wordcount[word] += 1\n",
    "        else:\n",
    "            wordcount[word] = 1\n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd82c0",
   "metadata": {},
   "source": [
    "p(the, panda) = count(the panda) / count(the) = 1/3 = .33\n",
    "\n",
    "p(a, sandwich) = count(a sandwich) / count(a) = 1/2\n",
    "\n",
    "p(in, the) = count(in the) / count(in) = 0/2 = 0\n",
    "\n",
    "p(panda, eats) = count(panda eats) / count(panda) = 1 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea82ca-2c1a-437a-824e-4620c4449a5b",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d797b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['the panda eats the sandwich in her pajamas',\n",
    "         'a sandwich likes a panda',\n",
    "         'the sandwich likes her panda in some pajamas']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc2d2c",
   "metadata": {},
   "source": [
    "([BOS], the) = 2/3\n",
    "([UNK], panda) = 1/5\n",
    "(likes, [UNK]) = 1/2 // her, pamajas, some\n",
    "([UNK], peacefully) = 0/5\n",
    "([UNK], [UNK]) = 2 / 5 // her pajamas, some pajamas\n",
    "(panda, [EOS]) = 1 / 3\n",
    "(likes, sandwich) = 0 / 2\n",
    "\n",
    "1. It give additional context in a bigram model. using these tokens expresses probabilities of words occuring at the beginning and the end of the sentnece. This eliminates unlikely bigrams across the end of a sentence from owhterwise occuring. \n",
    "2. The UNK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fd57d-96f3-4c25-ab34-eabcbb64bc2a",
   "metadata": {},
   "source": [
    "### Part 1.3\n",
    "|V| = 13\n",
    "k = 1\n",
    "\n",
    "text = ['the panda eats the sandwich in her pajamas',\n",
    "         'a sandwich likes a panda',\n",
    "         'the sandwich likes her panda in some pajamas']\n",
    "\n",
    "(the, panda) = (1+1)/(3+13*1) = .125  \n",
    "\n",
    "(a, sandwich) = (1+1)/(3+13*1) = 2/15\n",
    "\n",
    "(in, the) = 1/15\n",
    "\n",
    "(panda, eats) = 2/16 \n",
    "\n",
    "([BOS], the) = 3/16  \n",
    "([UNK], [UNK]) = 3/18 = 1/6  \n",
    "(panda, [EOS]) = 2/16 1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00491d7c-b24b-424a-a314-72add06b1283",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Use this part to test your code. Feel free to add any additional code chunks. Note, once you make changes to `Lab4.py` you might have to restart the kernel for the changes to be reflected here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41733e9b-9e0e-42ef-a958-39dd5600f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')))\n",
      "Expecting:\n",
      "    {1: 70, 2: 3}\n",
      "ok\n",
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')), print_non1=True)\n",
      "Expecting:\n",
      "    {2: [('kitten', 'had'), ('.', '[EOS]'), ('for', 'the')]}\n",
      "ok\n",
      "Trying:\n",
      "    len(getVocab('data/glove_vocab.txt'))\n",
      "Expecting:\n",
      "    400003\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=True)\n",
      "Expecting:\n",
      "    [['[BOS]', 'one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.', '[EOS]'], ['[BOS]', 'for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.', '[EOS]']]\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=False)\n",
      "Expecting:\n",
      "    [['one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.']]\n",
      "ok\n",
      "4 items had no tests:\n",
      "    Lab4\n",
      "    Lab4.TestBigramFreqs\n",
      "    Lab4.getBigramProb\n",
      "    Lab4.getUnigramFreqs\n",
      "3 items passed all tests:\n",
      "   2 tests in Lab4.getBigramFreqs\n",
      "   1 tests in Lab4.getVocab\n",
      "   2 tests in Lab4.preprocess\n",
      "5 tests in 7 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CHUNK\n",
    "import Lab4\n",
    "import doctest\n",
    "\n",
    "doctest.testmod(Lab4, verbose=True) ## runs the doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0618d5ee-276a-4eeb-9d1b-1afaeb6668d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'thing'): True | val: 1.0 | bigramprob: 1.0\n",
      "-------------------------------------------------\n",
      "('kitten', 'had'): True | val: 0.6666666666666666 | bigramprob: 0.6666666666666666\n",
      "-------------------------------------------------\n",
      "('cat', 'had'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('had', 'had'): True | val: 0.25 | bigramprob: 0.25\n",
      "-------------------------------------------------\n",
      "('on', 'the'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('held', 'a'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('zzzzzzz', 'the'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "## Correct probs to test your implementation of getBigramProb()\n",
    "correct_probs_mle = {\n",
    "        ('one', 'thing'): 1.0,\n",
    "        ('kitten', 'had'): 0.6666666666666666,\n",
    "        ('cat', 'had'): 0.0,\n",
    "        ('had', 'had'): 0.25,\n",
    "        ('on', 'the'): 0.0,\n",
    "        ('held', 'a'): 0.0,\n",
    "        ('zzzzzzz', 'the'): 0.0\n",
    "    }\n",
    "\n",
    "text = Lab4.preprocess('./data/test.txt', True)\n",
    "vocab = Lab4.getVocab('./data/glove_vocab.txt')\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "# print(f'vocab: {vocab}')\n",
    "# print(f'bigramfrq: {bigramfrq}')\n",
    "for key, val in correct_probs_mle.items():\n",
    "    bigramprob = Lab4.getBigramProb(key, 'MLE', bigramfrq = bigramfrq, unigramfrq = unigramfrq, vocab = vocab)\n",
    "    print(f\"{key}: {val == bigramprob} | val: {val} | bigramprob: {bigramprob}\")\n",
    "    print(f\"-------------------------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97052275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'thing'): True | val: 4.999950000499995e-06 | bigramprob: 4.999950000499995e-06\n",
      "-------------------------------------------------\n",
      "('kitten', 'had'): True | val: 7.499887501687475e-06 | bigramprob: 7.499887501687475e-06\n",
      "-------------------------------------------------\n",
      "('cat', 'had'): True | val: 2.4999750002499977e-06 | bigramprob: 2.4999750002499977e-06\n",
      "-------------------------------------------------\n",
      "('had', 'had'): True | val: 4.999912501531223e-06 | bigramprob: 4.999912501531223e-06\n",
      "-------------------------------------------------\n",
      "('on', 'the'): True | val: 2.499981250140624e-06 | bigramprob: 2.499981250140624e-06\n",
      "-------------------------------------------------\n",
      "('held', 'a'): True | val: 2.499981250140624e-06 | bigramprob: 2.499981250140624e-06\n",
      "-------------------------------------------------\n",
      "('zzzzzzz', 'the'): True | val: 2.4999562507656116e-06 | bigramprob: 2.4999562507656116e-06\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "correct_probs_add1 = {\n",
    "        ('one', 'thing'): 4.999950000499995e-06,\n",
    "        ('kitten', 'had'): 7.499887501687475e-06,\n",
    "        ('cat', 'had'): 2.4999750002499977e-06,\n",
    "        ('had', 'had'): 4.999912501531223e-06,\n",
    "        ('on', 'the'): 2.499981250140624e-06,\n",
    "        ('held', 'a'): 2.499981250140624e-06,\n",
    "        ('zzzzzzz', 'the'): 2.4999562507656116e-06\n",
    "    }\n",
    "\n",
    "text = Lab4.preprocess('./data/test.txt', True)\n",
    "vocab = Lab4.getVocab('./data/glove_vocab.txt')\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "\n",
    "for key, val in correct_probs_add1.items():\n",
    "    bigramprob = Lab4.getBigramProb(key, 'add-1', bigramfrq = bigramfrq, unigramfrq = unigramfrq, vocab = vocab)\n",
    "    print(f\"{key}: {val == bigramprob} | val: {val} | bigramprob: {bigramprob}\")\n",
    "    print(f\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ce58a-2b2d-4e3a-80fa-d1b54377585b",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful. **Remember, once you make changes to your `Lab4.py`, you need to restart the kernel and reimport the file**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee0bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLE] through_the_looking_glass: 0.2036300783960991\n",
      "[MLE] alice_in_wonderland: 0.12879269055987674\n",
      "[MLE] sherlock_holmes: 0.10245125709913011\n",
      "[Add-1] through_the_looking_glass: 0.015208706719513729\n",
      "[Add-1] alice_in_wonderland: 0.01501280905245055\n",
      "[Add-1] sherlock_holmes: 0.015891383188951003\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "ttlgf = './data/through_the_looking_glass.txt'\n",
    "aiw = './data/alice_in_wonderland.txt'\n",
    "sh = './data/sherlock_holmes.txt'\n",
    "\n",
    "# ttlgf\n",
    "vocab = Lab4.getTrainVocab(ttlgf)\n",
    "text = Lab4.preprocess(ttlgf, True)\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "\n",
    "\n",
    "# MLE\n",
    "a = Lab4.evaluate(ttlgf, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "b = Lab4.evaluate(aiw, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "c = Lab4.evaluate(sh, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "\n",
    "print(f'[MLE] through_the_looking_glass: {a}')\n",
    "print(f'[MLE] alice_in_wonderland: {b}')\n",
    "print(f'[MLE] sherlock_holmes: {c}')\n",
    "\n",
    "# Add-1\n",
    "d = Lab4.evaluate(ttlgf,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "e = Lab4.evaluate(aiw,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "f = Lab4.evaluate(sh,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "\n",
    "print(f'[Add-1] through_the_looking_glass: {d}')\n",
    "print(f'[Add-1] alice_in_wonderland: {e}')\n",
    "print(f'[Add-1] sherlock_holmes: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61b89f",
   "metadata": {},
   "source": [
    "When trained on `through the looking glass` without smoothing, when it's evaluating the text it was trained on a higher probability is assigned. This make sense since the vocab and the bigramfrq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f12f1a-e4ba-469a-9e47-22796546569c",
   "metadata": {},
   "source": [
    "## Part 4 (optional)\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
