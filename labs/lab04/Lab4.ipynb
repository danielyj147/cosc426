{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f397c0dc-1fed-4f5b-89a8-e0af21e06c0a",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab4.md`. Make sure to include in this notebook all the tests you run to ensure that at each stage your n-gram model implementation is correct. Please use **markdown** chunks to answer any non-code questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda0e4a-6302-4d4e-8c55-cc75a8648a0c",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Answer questions for each of the subparts here. Add as many markdown chunks as needed under each sub-part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef565601-0982-4932-a12d-290e1dd28b60",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d90c7e",
   "metadata": {},
   "source": [
    "vocab = {a, the, is, in, likes, eats, sandwich, panda, joyfully, peacefully}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318a12b",
   "metadata": {},
   "source": [
    "text = ['the panda eats the sandwich in her pajamas',  \n",
    "         'a sandwich likes a panda',  \n",
    "         'the sandwich likes her panda in some pajamas']  \n",
    "Unknown tokens = [her,pajamas, her, some, pajamas]\n",
    "Count of unknowns = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f000410",
   "metadata": {},
   "source": [
    "What probability would the model with the new vocabulary asssign to the the following bigrams?  \n",
    "\n",
    "P(the, panda) = count(the panda) / count(the) = 1/3  \n",
    "P(a, sandwich) = count(a sandwhich) / count(a) = 1/2\n",
    "P(in, the) = count(in the) / count(in) = 0 / 2 = 0  \n",
    "P(panda, eats) = count(panda eats) / count(panda) = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea82ca-2c1a-437a-824e-4620c4449a5b",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0795f",
   "metadata": {},
   "source": [
    "vocab = {a, the, is, in, likes, eats, sandwich, panda, joyfully, peacefully, [BOS], [EOS], [UNK]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950058b4",
   "metadata": {},
   "source": [
    "What probability would the model with the new vocabulary asssign to the the following bigrams?\n",
    "\n",
    "P([BOS], the) = count([BOS] the] / count([BOS]) = 2/3  \n",
    "P([UNK], panda) = count([UNK] panda] / count([UNK]) = 1/5  \n",
    "P(likes, [UNK]) = count(likes [UNK]) / count(likes) = 1/2  \n",
    "P([UNK], peacefully) = count([UNK] peacefully) / count([UNK]) = 0/5 = 0    \n",
    "P([UNK], [UNK]) = count([UNK] [UNK] / count([UNK]) = 2/5  (her pajamas, some pajamas)  \n",
    "P(panda, [EOS]) = count(panda [EOS]) / count(panda) = 1/3  \n",
    "P(likes, sandwich) = count(likes sandwich) / count(likes) = 0/2 = 0  \n",
    "\n",
    "1. Why is it useful to have [BOS] and [EOS] tokens?  \n",
    "   These types of tokens give additional context. In a bigram model, using these tokens expresses probabilities of words occuring at the beginning of end of the sentence. This eliminates certain unlikely bigrams across the end of a sentence from otherwise occuring (for example, without these tokens \"pajamas a\" would be a represented bigram which is not contextually useful).\n",
    "   \n",
    "3. Why is it a useful strategy to replace words not in the vocabulary with the [UNK] token?  \n",
    "    The [UNK] token eliminates numerical issues with calculating probabilities. Without these tokens, unseen words would initially be assigned a 0 probability and could cause divide by 0 issues when calculating probability. Additionally, it assigns more realsitic probabilities for bigrams with rare words (in which the first part of a bigram is common but the second word is not). An example of this is for this vocabulary is if we had the bigram \"the apple\". The model would assign a 0 probability to this but the UNK token allows for processing of this phrase and gives it a higher probabilitiy (because the token \"the\" is common)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fd57d-96f3-4c25-ab34-eabcbb64bc2a",
   "metadata": {},
   "source": [
    "### Part 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b126ca0",
   "metadata": {},
   "source": [
    "vocab = {a, the, is, in, likes, eats, sandwich, panda, joyfully, peacefully, [UNK],[BOS],[EOS]}  \n",
    "|V| = 13\n",
    "k = 1 -> add 1 smoothing\n",
    "\n",
    "text = ['the panda eats the sandwich in her pajamas',  \n",
    "         'a sandwich likes a panda',  \n",
    "         'the sandwich likes her panda in some pajamas']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f01a2c",
   "metadata": {},
   "source": [
    "What probability would this new model assign to the following bigrams?\n",
    "\n",
    "P(the, panda) = Count(the panda) + 1 / Count(the) + |V|*1 = (1+1) / (3+13) = 2/16 = 1/8  \n",
    "P(a, sandwich) = Count(a sandwich) + 1 / Count(a) + |V|*1 = (1+1) / (2+13) = 2/15  \n",
    "P(in, the) = Count(in the) + 1 / Count(in) + |V|*1 = (0+1) / (2+13) = 1/15 = 1/15 \n",
    "P(panda, eats) = Count(panda eats) + 1 / Count(panda) + |V|*1 = (1+1) / (3+13) = 2/16 = 1/8  \n",
    "P([BOS], the) = Count([BOS] the) + 1 / Count([BOS]) + |V|*1 = (2+1) / (3+13) = 3/16\n",
    "P([UNK], [UNK]) = Count([UNK] [UNK]) + 1 / Count([UNK]) + |V|*1 = (2+1) / (5+13) = 3/18 = 1/6  \n",
    "P(panda, [EOS]) = Count(panda [EOS]) + 1 / Count(panda) + |V|*1 = (1+1) / (3+13) = 2/16 = 1/8  \n",
    "\n",
    "1. What is the motivation for smoothing? If it is easier to articulate your answer, describe a scenario where the absence of smoothing is a problem.\n",
    "   Smoothing allows a model to create a dilutive affect on initial probabilities. Without smoothing, the model only calculates probabilities based on the text it has seen. In reality, you want the model to do well and calculate probabilities according to new data (and possible unknown words/bigrams) it has not seen. For example, if a training set is very close to a set you want to evaluate, you do not need a large amount of smoothing since most of the bigrams and tokens will be representative of previously seen data. However, if your model was trained on wikipedia and you give it a text of Shakespeare, you want a larger dillutive effect in order to account for the large number of previously unseen bigrams.  \n",
    "   \n",
    "3. What happens to the probability of observed bigrams with smoothing?  \n",
    "    The probability of observed bigrams decreases as a result of smoothing. This is neccessary to allow each possible bigram to have nonzero probability (as a result the seen bigrams probability will slightly decrease). For example, in 1.2 P(Panda [EOS]) = 1/3, but in 1.3, P(Panda [EOS]) decreases dramatically to 1/8.  \n",
    "5. Which of the following smoothing types will be closest to the MLE estimate (i.e., unsmoothed estimate) for observed bigrams? add-1, add-2, add-0.1?  \n",
    "    The lower the k-value (what you are adding to each bigram), the less diluted of an affect it will have on unseen tokens. For example, if you multily |V| by a smaller number in the denominator, it will have less of an impact in changing the probability of a seen bigram. Obviously, unseen bigrams with smoothing are given higher probabilities.\n",
    "\n",
    "   \n",
    "7. If the data that you are testing on is very similar to the data used to train your bigram model, would you want to use higher or lower values of k in add-k smoothing? Why?\n",
    "    If we are testing with similar data/text to the training set, we would want much lower k values. If a lot of the text (bigrams and tokens) have already been seen, you want less of a diluted affect on these probabilities. As explained in question 3, a lower k value has a less diluted affect on seen bigrams. Because the testing set is similar to the training set, a lot of the bigrams and tokens will be seen, so we want the probabilities of these values to be as close as possible a representation without dilution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00491d7c-b24b-424a-a314-72add06b1283",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Use this part to test your code. Feel free to add any additional code chunks. Note, once you make changes to `Lab4.py` you might have to restart the kernel for the changes to be reflected here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41733e9b-9e0e-42ef-a958-39dd5600f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')))\n",
      "Expecting:\n",
      "    {1: 70, 2: 3}\n",
      "ok\n",
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')), print_non1=True)\n",
      "Expecting:\n",
      "    {2: [('kitten', 'had'), ('.', '[EOS]'), ('for', 'the')]}\n",
      "ok\n",
      "Trying:\n",
      "    len(getVocab('data/glove_vocab.txt'))\n",
      "Expecting:\n",
      "    400003\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=True)\n",
      "Expecting:\n",
      "    [['[BOS]', 'one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.', '[EOS]'], ['[BOS]', 'for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.', '[EOS]']]\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=False)\n",
      "Expecting:\n",
      "    [['one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.']]\n",
      "ok\n",
      "4 items had no tests:\n",
      "    Lab4\n",
      "    Lab4.TestBigramFreqs\n",
      "    Lab4.getBigramProb\n",
      "    Lab4.getUnigramFreqs\n",
      "3 items passed all tests:\n",
      "   2 tests in Lab4.getBigramFreqs\n",
      "   1 tests in Lab4.getVocab\n",
      "   2 tests in Lab4.preprocess\n",
      "5 tests in 7 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CHUNK\n",
    "import Lab4\n",
    "import doctest\n",
    "\n",
    "doctest.testmod(Lab4, verbose=True) ## runs the doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618d5ee-276a-4eeb-9d1b-1afaeb6668d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'thing'): True | val: 1.0 | bigramprob: 1.0\n",
      "-------------------------------------------------\n",
      "('kitten', 'had'): True | val: 0.6666666666666666 | bigramprob: 0.6666666666666666\n",
      "-------------------------------------------------\n",
      "('cat', 'had'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('had', 'had'): True | val: 0.25 | bigramprob: 0.25\n",
      "-------------------------------------------------\n",
      "('on', 'the'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('held', 'a'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n",
      "('zzzzzzz', 'the'): True | val: 0.0 | bigramprob: 0.0\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "## Correct probs to test your implementation of getBigramProb()\n",
    "correct_probs_mle = {\n",
    "        ('one', 'thing'): 1.0,\n",
    "        ('kitten', 'had'): 0.6666666666666666,\n",
    "        ('cat', 'had'): 0.0,\n",
    "        ('had', 'had'): 0.25,\n",
    "        ('on', 'the'): 0.0,\n",
    "        ('held', 'a'): 0.0,\n",
    "        ('zzzzzzz', 'the'): 0.0\n",
    "    }\n",
    "\n",
    "text = Lab4.preprocess('./data/test.txt', True)\n",
    "vocab = Lab4.getVocab('./data/glove_vocab.txt')\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "\n",
    "for key, val in correct_probs_mle.items():\n",
    "    bigramprob = Lab4.getBigramProb(key, 'MLE', bigramfrq = bigramfrq, unigramfrq = unigramfrq, vocab = vocab)\n",
    "    print(f\"{key}: {val == bigramprob} | val: {val} | bigramprob: {bigramprob}\")\n",
    "    print(f\"-------------------------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97052275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'thing'): True | val: 4.999950000499995e-06 | bigramprob: 4.999950000499995e-06\n",
      "-------------------------------------------------\n",
      "('kitten', 'had'): True | val: 7.499887501687475e-06 | bigramprob: 7.499887501687475e-06\n",
      "-------------------------------------------------\n",
      "('cat', 'had'): True | val: 2.4999750002499977e-06 | bigramprob: 2.4999750002499977e-06\n",
      "-------------------------------------------------\n",
      "('had', 'had'): True | val: 4.999912501531223e-06 | bigramprob: 4.999912501531223e-06\n",
      "-------------------------------------------------\n",
      "('on', 'the'): True | val: 2.499981250140624e-06 | bigramprob: 2.499981250140624e-06\n",
      "-------------------------------------------------\n",
      "('held', 'a'): True | val: 2.499981250140624e-06 | bigramprob: 2.499981250140624e-06\n",
      "-------------------------------------------------\n",
      "('zzzzzzz', 'the'): True | val: 2.4999562507656116e-06 | bigramprob: 2.4999562507656116e-06\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "correct_probs_add1 = {\n",
    "        ('one', 'thing'): 4.999950000499995e-06,\n",
    "        ('kitten', 'had'): 7.499887501687475e-06,\n",
    "        ('cat', 'had'): 2.4999750002499977e-06,\n",
    "        ('had', 'had'): 4.999912501531223e-06,\n",
    "        ('on', 'the'): 2.499981250140624e-06,\n",
    "        ('held', 'a'): 2.499981250140624e-06,\n",
    "        ('zzzzzzz', 'the'): 2.4999562507656116e-06\n",
    "    }\n",
    "\n",
    "text = Lab4.preprocess('./data/test.txt', True)\n",
    "vocab = Lab4.getVocab('./data/glove_vocab.txt')\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "\n",
    "for key, val in correct_probs_add1.items():\n",
    "    bigramprob = Lab4.getBigramProb(key, 'add-1', bigramfrq = bigramfrq, unigramfrq = unigramfrq, vocab = vocab)\n",
    "    print(f\"{key}: {val == bigramprob} | val: {val} | bigramprob: {bigramprob}\")\n",
    "    print(f\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ce58a-2b2d-4e3a-80fa-d1b54377585b",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful. **Remember, once you make changes to your `Lab4.py`, you need to restart the kernel and reimport the file**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee0bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLE] through_the_looking_glass: 0.2036300783960991\n",
      "[MLE] alice_in_wonderland: 0.12879269055987674\n",
      "[MLE] sherlock_holmes: 0.10245125709913011\n",
      "[Add-1] through_the_looking_glass: 0.015208706719513729\n",
      "[Add-1] alice_in_wonderland: 0.01501280905245055\n",
      "[Add-1] sherlock_holmes: 0.015891383188951003\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "\n",
    "ttlgf = './data/through_the_looking_glass.txt'\n",
    "aiw = './data/alice_in_wonderland.txt'\n",
    "sh = './data/sherlock_holmes.txt'\n",
    "\n",
    "# ttlgf\n",
    "vocab = Lab4.getTrainVocab(ttlgf)\n",
    "text = Lab4.preprocess(ttlgf, True)\n",
    "bigramfrq = Lab4.getBigramFreqs(text, vocab)\n",
    "unigramfrq = Lab4.getUnigramFreqs(text, vocab)\n",
    "\n",
    "\n",
    "# MLE\n",
    "a = Lab4.evaluate(ttlgf, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "b = Lab4.evaluate(aiw, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "c = Lab4.evaluate(sh, 'MLE', bigramfrq, unigramfrq, vocab)\n",
    "\n",
    "print(f'[MLE] through_the_looking_glass: {a}')\n",
    "print(f'[MLE] alice_in_wonderland: {b}')\n",
    "print(f'[MLE] sherlock_holmes: {c}')\n",
    "\n",
    "# Add-1\n",
    "d = Lab4.evaluate(ttlgf,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "e = Lab4.evaluate(aiw,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "f = Lab4.evaluate(sh,'add-1', bigramfrq, unigramfrq, vocab)\n",
    "\n",
    "print(f'[Add-1] through_the_looking_glass: {d}')\n",
    "print(f'[Add-1] alice_in_wonderland: {e}')\n",
    "print(f'[Add-1] sherlock_holmes: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61b89f",
   "metadata": {},
   "source": [
    "When evaluating the trained model on the three different texts with no smoothing, the highest probability was assigned to the evaluated text that was the same as the trained text (“through_the_looking_glass.txt). This makes sense because with no smoothing effect, higher probabilities should be assigned to bigrams that have already been seen. Since every bigram has already been seen in the training set, evaluating on the same set should have a higher probability than evaluating on the other two texts. It also makes sense that \"alice_in_wonderland.txt\" has a higher probability than \"sherlock_holmes.txt\" but lower than the training text. This is because \"through_the_looking_glass.txt\" and \"alice_in_wonderland.txt\" have the same author, so without smoothing there should be more shared tokens between those two texts compared to \"sherlock_holmes.txt\".\n",
    "When a smoothing effect is added, since the dilution constant k=1 is somewhat large, each bigram from the training set is assigned a lower probability when it is evaluated. This causes the probability of the (“through_the_looking_glass.txt”) to be lower when add-1 smoothing is used. Compared to the other texts, although the other two texts have more unseen bigrams and tokens, the smoothing effect causes a large number of bigrams to be categorized together as [UNK]. The probability of seeing [UNK] tokens after one another is consequently somewhat large, so the diluting effect actually makes seeing these two texts more likely. As a result of this series of effects occurring to different degrees between each text, “through_the_looking_glass.txt” is assigned a lower probability than “sherlock_holmes.txt” when evaluated on an add-1 bigram model. However, “through_the_looking_glass.txt” still has a slightly higher cumulative average likelihood than “alice_in_wonderland.txt”. This may occur because of the relationship between the two texts. Because these two texts have the same author, there are less [UNK] tokens compared to \"sherlock_holmes.txt\". As a result, the relationship between counted frequencies and the dilutive effect of smoothing is comparatively less severe, so \"alice_in_wonderland\" actually becomes less probabilistic than the training text despite the large amount of smoothing occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f12f1a-e4ba-469a-9e47-22796546569c",
   "metadata": {},
   "source": [
    "## Part 4 (optional)\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725dd6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/jeong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Evaluation on: ./data/through_the_looking_glass.txt\n",
      "======================================================================\n",
      "[      add-2][mark the ends: true] -> Score: 0.0087\n",
      "[      add-2][mark the ends:false] -> Score: 0.0042\n",
      "----------------------------------------------------------------------\n",
      "[      add-1][mark the ends: true] -> Score: 0.0152\n",
      "[      add-1][mark the ends:false] -> Score: 0.0073\n",
      "----------------------------------------------------------------------\n",
      "[    add-0.1][mark the ends: true] -> Score: 0.0543\n",
      "[    add-0.1][mark the ends:false] -> Score: 0.0281\n",
      "----------------------------------------------------------------------\n",
      "[   add-0.01][mark the ends: true] -> Score: 0.0942\n",
      "[   add-0.01][mark the ends:false] -> Score: 0.0600\n",
      "----------------------------------------------------------------------\n",
      "[  add-0.001][mark the ends: true] -> Score: 0.1392\n",
      "[  add-0.001][mark the ends:false] -> Score: 0.1039\n",
      "----------------------------------------------------------------------\n",
      "[ add-0.0001][mark the ends: true] -> Score: 0.1853\n",
      "[ add-0.0001][mark the ends:false] -> Score: 0.1498\n",
      "----------------------------------------------------------------------\n",
      "[  add-1e-05][mark the ends: true] -> Score: 0.2013\n",
      "[  add-1e-05][mark the ends:false] -> Score: 0.1658\n",
      "======================================================================\n",
      "Evaluation Complete.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import Lab4\n",
    "ttlgf = './data/through_the_looking_glass.txt'\n",
    "\n",
    "Lab4.run_all_tests(ttlgf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
