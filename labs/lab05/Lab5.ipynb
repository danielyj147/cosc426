{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3356241a-7e20-42d3-8f00-7f6707b65b5f",
   "metadata": {},
   "source": [
    "# Lab 5: Bayesian Classification\n",
    "### COSC 426: Fall 2025, Colgate University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c04158-1b47-4004-ae8f-1c0e9196de01",
   "metadata": {},
   "source": [
    "## Part 1: Build a unigram model\n",
    "\n",
    "#### Part 1.1\n",
    "\n",
    "Start by understanding what is happening when you initilize a UnigramModel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55c9ea-afde-4395-88a1-25a9ef8f0727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeong/Projects/cosc426-projects/cosc426/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from UnigramModel import UnigramModel\n",
    "import util\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sample_model = UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d39c6-b2ec-4a61-93f2-34136ccee179",
   "metadata": {},
   "source": [
    "**What are the parameters required to initialize a `UnigramModel` object and how are these parameters used in the `UnigramModel` class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7a210-9f67-4c49-9564-85f5fc5c1377",
   "metadata": {},
   "source": [
    "#### Part 1.2\n",
    "\n",
    "Verify your implementation of `get_prob` and `evaluate` with the code below. \n",
    "\n",
    "*Hint: Running into errors \"Rabbit\" in `get_probs`? Think carefully about when are where preprocessing is being applied in the pipeline, and what the expected input for `get_probs` is*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73eeece",
   "metadata": {},
   "source": [
    "We need tokenizer, vocab(with unknown token), train data, smoothing stretegy.\n",
    "They all have default values. Tokeniger is used to split text in to tokens. Vocab is used to mark whether a work is known or not. Train data is the text we use to find n-gram frequencies. `smooth` is used to indiciate the kind of smoothing strategy we use get the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c3e702-a3b5-4500-bc97-db4ecfaffb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are dfs same? True\n"
     ]
    }
   ],
   "source": [
    "## print prob of words\n",
    "expected_probs = {'rabbit': 0.00010176146615934851,\n",
    "                  'it': 0.0002755005547240899,\n",
    "                  '[UNK]': 0.00017622107554423767,\n",
    "                  'Alice': 0.00017622107554423767,\n",
    "                  'Rabbit': 0.00017622107554423767,\n",
    "                  'maze': 2.4819869794963054e-06\n",
    "                 }\n",
    "\n",
    "for word in expected_probs:\n",
    "    if sample_model.get_prob(word) != expected_probs[word]:\n",
    "        print(word, '\\t incorrect')\n",
    "        print('expected', expected_probs[word])\n",
    "        print('got', sample_model.get_prob(word))\n",
    "        print()\n",
    "        \n",
    "\n",
    "## Create paths and then load it\n",
    "sample_model.evaluate(datafpath= 'data/sample-alice.txt',\n",
    "                      predfpath = 'predictions/my_sample_preds_alice.tsv')\n",
    "\n",
    "\n",
    "\n",
    "correct_df = pd.read_csv('predictions/my_sample_preds_alice.tsv', sep='\\t')\n",
    "my_df = pd.read_csv('predictions/sample_preds_alice.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "# Does element wise comparison\n",
    "print('Are dfs same?', correct_df.equals(my_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5100a57-2064-4b98-a324-a170c0f7a588",
   "metadata": {},
   "source": [
    "## Part 2: Implement building blocks of a Naive Bayesian Classifier\n",
    "\n",
    "Implement the building blocks for a Bayesian classifier. Here is a function that might be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b6f41-3deb-439d-a3a4-fb2700893aab",
   "metadata": {},
   "source": [
    "### Part 2.1: Describe your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f20b5",
   "metadata": {},
   "source": [
    "p(class | text) ~ p(text | class) * p(class)\n",
    "\n",
    "How is unigram model related to likelihood?  \n",
    "p(text | class) is probability of the word from the unigram model trained on the class text.\n",
    "\n",
    "We can get p(text|class) by useing the unigram model trained on the class text, then we calculate p(class) using the 1/number of class. When we multiply the `likelihood` and the `prior`, we get the probability of the class given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b94e34-9d46-4f7f-9355-80d8e547eec7",
   "metadata": {},
   "source": [
    "### Part 2.2 Implement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c03a902-94c0-494e-958b-0cdd34350c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def summarize(fname:str, aggregrate_type:str, aggregrate_col:str, groupby_cols:list, delimiter='\\t'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fname: fpath to tsv/ csv file\n",
    "        aggregate_type: mean or sum\n",
    "\n",
    "        aggrefate_col: the column with values you want to aggregate over\n",
    "\n",
    "        groupby_cols: the columns with the groups. \n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe with as many rows as unique group combinations. The values of rows in each group is either summed together or averaged depending on the aggregate_type. \n",
    "\n",
    "    \"\"\"\n",
    "    dat = pd.read_csv(fname, sep=delimiter)\n",
    "\n",
    "    summ = dat.groupby(groupby_cols).agg({aggregrate_col: aggregrate_type}).reset_index()\n",
    "\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689d474-0107-4f15-954a-3f726fc06d78",
   "metadata": {},
   "source": [
    "#### **Calculating the likelihood**\n",
    "\n",
    "Start by calculating the likelihood of some text given models trained on text from different classes --- i.e., $P(text \\mid model=class1)$, $P(text \\mid model=class2)$, etc\n",
    "\n",
    "*Hint: Think about why `summarize` function provided is useful* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79577b49-c040-4df9-83c9-dc23c2a165b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(models_dict:dict, eval_fpath, class_label):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        models_dict: keys are classes and values are the models trained on the classes\n",
    "        eval_fpath: the file models should be evaluated on\n",
    "        class_label: the correct class label for sequences in the file \n",
    "\n",
    "    Returns:\n",
    "        A Dataframe with the following columns: \n",
    "            sentid: id of the sentence\n",
    "            model: the model being used to generate the likelihood\n",
    "            likelihood: the sum of log probability across all the words in the sequence\n",
    "            target_class: same as class_label\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    text = models_dict[class_label].preprocess([eval_fpath])\n",
    "    rows = []\n",
    "    for model, um in models_dict.items():\n",
    "        for sentid in range(len(text)):\n",
    "            sent = text[sentid]\n",
    "            if sent:\n",
    "                    likelihood = 0\n",
    "                    for wordpos in range(len(sent)):\n",
    "                        word = sent[wordpos]\n",
    "                        prob = um.get_prob(word)\n",
    "                        surp = math.log2(prob)\n",
    "                        likelihood += surp\n",
    "                    rows.append([sentid, model, likelihood, class_label])\n",
    "\n",
    "    # df = pd.DataFrame(rows, columns=[\"sentid\", \"model\", \"likelihood\", \"class_label\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"sentid\", \"model\", \"likelihood\", \"target_class\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9474e0-d9a6-42d7-a2b3-9386be26d280",
   "metadata": {},
   "source": [
    "Once you've implemented this function, verify that your output matches the expected output below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58a3705-d506-400b-a186-e10e9e382085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing proportion of matched values across correct_df and my_df\n",
      "\n",
      "likelihood 1.0\n",
      "sentid 1.0\n",
      "model 1.0\n",
      "target_class 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_models = {\n",
    "    'alice': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           ),\n",
    "    'sherlock': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-sherlock.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )\n",
    "}\n",
    "\n",
    "my_df = get_likelihood(sample_models, 'data/sample-lookingglass.txt', 'alice').reset_index(drop=True)\n",
    "correct_df = pd.read_csv('predictions/sample-likelihood.tsv', sep='\\t').reset_index(drop=True)\n",
    "\n",
    "# using this instead of equal because of floating point imprecision\n",
    "print('Printing proportion of matched values across correct_df and my_df\\n')\n",
    "print('likelihood', np.isclose(my_df['likelihood'], correct_df['likelihood']).sum()/len(my_df)) \n",
    "for col in ['sentid', 'model', 'target_class']:\n",
    "    print(col, (my_df[col] == correct_df[col]).sum()/len(my_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21711e91-0b7f-449e-a7d8-dde7227f8f84",
   "metadata": {},
   "source": [
    "#### Calculating the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09daf921-49e0-4e3f-8ff3-8d0b8307d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(data: dict, tokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: dictionary where keys are the classes, and values are filepaths to the class specific data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prior probability for each class, which is the number of words in the class divided by the total number of words across all classes. \n",
    "        What counts as a word is determined by the tokenizer\n",
    "\n",
    "    \"\"\"\n",
    "    n = 0 # total word count\n",
    "    cwc = {} # calss word count: [class]: num\n",
    "    for cls, files in data.items():\n",
    "        count = 0\n",
    "        cwc[cls] = 0\n",
    "\n",
    "        for f in files:\n",
    "            with open(f, \"r\") as file:\n",
    "                text = file.readlines()\n",
    "\n",
    "        for sent in text:\n",
    "            curr_dat = tokenizer(sent, {})\n",
    "            count += len(curr_dat)\n",
    "            cwc[cls] += len(curr_dat)\n",
    "\n",
    "        n += count\n",
    "\n",
    "    # compute prior probabilities\n",
    "    prior = {key: val / n for key, val in cwc.items()}\n",
    "    return prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da052a32-3251-4fa5-8290-4e166b20fedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sherlock': 0.4423076923076923, 'alice': 0.5576923076923077}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict = {'sherlock': ['data/sample-sherlock.txt'],\n",
    "            'alice': ['data/sample-alice.txt']}\n",
    "\n",
    "correct_prior = {'sherlock': 0.4423076923076923, 'alice': 0.5576923076923077}\n",
    "get_prior(dat_dict, util.nltk_tokenize)  # should match correct_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a096-f34d-4eaf-bdd5-5168161d0253",
   "metadata": {},
   "source": [
    "#### Compute posterior\n",
    "\n",
    "*Hint: Think about why `summarize` function provided is useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfa1a27-5fde-416f-8b86-2c4eb18edefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(models_dict, eval_fpath, class_label, prior_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Dictionary where keys are classes and values are the models trained on the classes\n",
    "\n",
    "        eval_fpath: the file models should be evaluated on. \n",
    "\n",
    "        class_label: the label of the file that models are evaluated on\n",
    "\n",
    "        prior_dict: prior probabilities of classes\n",
    "\n",
    "    Returns:\n",
    "        A Dataframe with the following columns: sentid, model, likelihood, class. \n",
    "\n",
    "    If you set eval_fpath to sample_reviews_test_positive.txt, you should get a dataframe that looks like this. (Its ok if you end up having additional columns)\n",
    "\n",
    "   sentid model     likelihood   class        prior      posterior\n",
    "       0  positive  -100.975898  positive    -0.736966   -101.712864\n",
    "       1  positive  -100.941133  positive    -0.736966   -101.678099\n",
    "       0  negative  -101.938780  positive    -1.321928   -103.260708\n",
    "       1  negative  -101.938780  positive    -1.321928   -103.260708\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    text = models_dict[class_label].preprocess(eval_fpath)\n",
    "    rows = []\n",
    "    for model, um in models_dict.items():\n",
    "        for sentid in range(len(text)):\n",
    "            sent = text[sentid]\n",
    "            if sent:\n",
    "                    likelihood = 0\n",
    "                    for wordpos in range(len(sent)):\n",
    "                        word = sent[wordpos]\n",
    "                        prob = um.get_prob(word)\n",
    "                        surp = math.log2(prob)\n",
    "                        likelihood += surp\n",
    "                    prior = math.log2(prior_dict[model])\n",
    "                    posterior = likelihood + prior\n",
    "                    rows.append([sentid, model, likelihood, class_label, prior, posterior])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"sentid\", \"model\", \"likelihood\", \"class\", \"prior\", \"posterior\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3072a5c3-f810-401c-8fa1-2450a2983d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior 1.0\n"
     ]
    }
   ],
   "source": [
    "my_df = get_posterior(\n",
    "    sample_models,\n",
    "    [\"data/sample-lookingglass.txt\"],\n",
    "    \"alice\",\n",
    "    get_prior(dat_dict, util.nltk_tokenize),\n",
    ").reset_index(drop=True)\n",
    "\n",
    "correct_df = pd.read_csv('predictions/sample-posterior.tsv', sep='\\t').reset_index(drop=True)\n",
    "print('posterior', np.isclose(my_df['posterior'], correct_df['posterior']).sum()/len(my_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042209e7-e0b5-48f0-938e-9fa8ad00573a",
   "metadata": {},
   "source": [
    "#### Implement classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c6cd5e-850f-4235-8c10-284d4040d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(posterior):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Dataframe with posterior probabilities\n",
    "\n",
    "    Returns: \n",
    "        Dataframe where each sentence id is associated with a prediction. \n",
    "    \"\"\"\n",
    "\n",
    "    classes = posterior['model'].unique()\n",
    "    wide_df = posterior.pivot(index=['sentid', 'class'],\n",
    "                              columns=['model'],\n",
    "                              values='posterior').reset_index()    \n",
    "    wide_df['pred'] = wide_df[classes].idxmax(axis=1) # finding the highest class.\n",
    "    return wide_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fab057-a729-4dad-b951-079b2295cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred 1.0\n"
     ]
    }
   ],
   "source": [
    "posterior = get_posterior(sample_models, \n",
    "            ['data/sample-lookingglass.txt'],\n",
    "            'alice',\n",
    "             get_prior(dat_dict, util.nltk_tokenize)).reset_index(drop=True)\n",
    "my_df = classify(posterior).reset_index(drop=True)\n",
    "\n",
    "correct_df = pd.read_csv('predictions/sample-classify.tsv', sep='\\t').reset_index(drop=True)\n",
    "print('pred', (my_df['pred']==correct_df['pred']).sum()/len(my_df)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d7d31-6c44-4ec8-a51f-4ceb5adf8e96",
   "metadata": {},
   "source": [
    "#### Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af4294d-9418-41da-8386-8b2e26d1be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(models_dict, eval_dict, prior_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        models_dict: keys are classes, values are models trained on data from the class. \n",
    "\n",
    "        eval_dict: keys are classes, values are fpaths to evaluation data where the correct label is the class associated with the key\n",
    "\n",
    "        prior_dict: keys are classes, values are prior probabilties of the classes. \n",
    "\n",
    "    Returns:\n",
    "        Float which is the accuracy of the predictions across all classes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_posteriors = []\n",
    "    for cls, fpath in eval_dict.items():\n",
    "        posterior = get_posterior(models_dict, fpath, cls, prior_dict)\n",
    "        all_posteriors.append(posterior)\n",
    "\n",
    "    all_posteriors_df = pd.concat(all_posteriors).reset_index(drop=True)\n",
    "    classified_df = classify(all_posteriors_df).reset_index(drop=True)\n",
    "\n",
    "    accuracy = (classified_df['pred'] == classified_df['class']).sum() / len(classified_df)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7d2694-ba6e-4476-92f4-e68bb294f872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9285714285714286)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_models = {\n",
    "    'alice': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           ),\n",
    "    'sherlock': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-sherlock.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )\n",
    "}\n",
    "\n",
    "## for simplicity making eval the same as train\n",
    "sample_eval = {\n",
    "    'alice': ['data/sample-lookingglass.txt'],\n",
    "    'sherlock': ['data/sample-sherlock.txt']\n",
    "}\n",
    "\n",
    "sample_prior = get_prior({'sherlock': ['data/sample-sherlock.txt'],\n",
    "                          'alice': ['data/sample-alice.txt']}, util.nltk_tokenize)\n",
    "\n",
    "\n",
    "target_acc = 92.857\n",
    "\n",
    "analyze(sample_models, sample_eval, sample_prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8cc45-0f9e-45ab-8ff0-bfb2d7a10f92",
   "metadata": {},
   "source": [
    "## Part 3: Build Naive Bayesian Sentiment Classifier\n",
    "Add as many code and markdown chunks as is helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a3941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_dpath = 'data/aclImdb/train/pos'\n",
    "neg_dpath = 'data/aclImdb/train/neg'\n",
    "\n",
    "post_fpaths = [os.path.join(pos_dpath, fname) for fname in os.listdir(pos_dpath)]\n",
    "neg_fpaths = [os.path.join(neg_dpath, fname) for fname in os.listdir(neg_dpath)]\n",
    "\n",
    "imdb_models = {\n",
    "    \"pos\": UnigramModel(\n",
    "        tokenize=util.nltk_tokenize,\n",
    "        tokenizer_kwargs={},\n",
    "        vocab=util.get_vocab(\"data/aclImdb/imdb.vocab\"),\n",
    "        unk_token=\"[UNK]\",\n",
    "        train_paths=post_fpaths,\n",
    "        smooth=\"add-0.1\",\n",
    "        lower=True,\n",
    "    ),\n",
    "    \"neg\": UnigramModel(\n",
    "        tokenize=util.nltk_tokenize,\n",
    "        tokenizer_kwargs={},\n",
    "        vocab=util.get_vocab(\"data/aclImdb/imdb.vocab\"),\n",
    "        unk_token=\"[UNK]\",\n",
    "        train_paths=neg_fpaths,\n",
    "        smooth=\"add-0.1\",\n",
    "        lower=True,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e21d897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 0.7658227848101266, 'neg': 0.23417721518987342}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.80708)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions from the model on the test set\n",
    "pos_test_dpaths = \"data/aclImdb/test/pos\"\n",
    "pos_test_fpaths = [\n",
    "    os.path.join(pos_test_dpaths, fname)\n",
    "    for fname in os.listdir(pos_test_dpaths)\n",
    "]\n",
    "\n",
    "neg_test_dpaths = \"data/aclImdb/test/neg\"\n",
    "neg_test_fpaths = [\n",
    "    os.path.join(neg_test_dpaths, fname)\n",
    "    for fname in os.listdir(neg_test_dpaths)\n",
    "]\n",
    "\n",
    "imdb_eval = {\n",
    "    \"pos\": pos_test_fpaths,\n",
    "    \"neg\": neg_test_fpaths,\n",
    "}\n",
    "\n",
    "imdb_prior = get_prior(\n",
    "    {\"pos\": pos_test_fpaths, \"neg\": neg_test_fpaths},\n",
    "    util.nltk_tokenize,\n",
    ")\n",
    "print(imdb_prior)\n",
    "analyze(imdb_models, imdb_eval, imdb_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44889ea1",
   "metadata": {},
   "source": [
    "An 80.7% accuracy is significantly higher than that of a random guess (50%), so the model has successfully learned the positive and negative sentiments. This is quite surprising to me since we are using a simple unigram model that only relies on single words. My assumption as to what contributed to this high accuracy is that I used \"add-0.1\" as the smoothing method instead of a higher k-value. I did so because the evaluation dataset was from the same repository of text (movie critiques), so I was confident that the training and evaluation sets would be very similar.\n",
    "\n",
    "However, there is a clear limit to the model, since it cannot account for a negated expression such as, \"This movie was not a great, wonderful, brilliant film.\" Since there are more positive words than negative words, it would certainly evaluate the sentence as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3a0e5-504c-438a-8228-af42cb09007b",
   "metadata": {},
   "source": [
    "## Part 4 (optional): Build Bigram Bayesian Sentiment Classifier\n",
    "Add as many code and markdown chunks as is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bd3e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
