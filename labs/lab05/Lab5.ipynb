{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3356241a-7e20-42d3-8f00-7f6707b65b5f",
   "metadata": {},
   "source": [
    "# Lab 5: Bayesian Classification\n",
    "### COSC 426: Fall 2025, Colgate University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c04158-1b47-4004-ae8f-1c0e9196de01",
   "metadata": {},
   "source": [
    "## Part 1: Build a unigram model\n",
    "\n",
    "#### Part 1.1\n",
    "\n",
    "Start by understanding what is happening when you initilize a UnigramModel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa55c9ea-afde-4395-88a1-25a9ef8f0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UnigramModel import UnigramModel\n",
    "import util\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "sample_model = UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d39c6-b2ec-4a61-93f2-34136ccee179",
   "metadata": {},
   "source": [
    "**What are the parameters required to initialize a `UnigramModel` object and how are these parameters used in the `UnigramModel` class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7a210-9f67-4c49-9564-85f5fc5c1377",
   "metadata": {},
   "source": [
    "#### Part 1.2\n",
    "\n",
    "Verify your implementation of `get_prob` and `evaluate` with the code below. \n",
    "\n",
    "*Hint: Running into errors \"Rabbit\" in `get_probs`? Think carefully about when are where preprocessing is being applied in the pipeline, and what the expected input for `get_probs` is*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73eeece",
   "metadata": {},
   "source": [
    "We need tokenizer, vocab(with unknown token), train data, smoothing stretegy.\n",
    "They all have default values. Tokeniger is used to split text in to tokens. Vocab is used to mark whether a work is known or not. Train data is the text we use to find n-gram frequencies. `smooth` is used to indiciate the kind of smoothing strategy we use get the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c3e702-a3b5-4500-bc97-db4ecfaffb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbit \t incorrect\n",
      "expected 0.00010176146615934851\n",
      "got 0.00010249923125576556\n",
      "\n",
      "it \t incorrect\n",
      "expected 0.0002755005547240899\n",
      "got 0.00027749791876560925\n",
      "\n",
      "[UNK] \t incorrect\n",
      "expected 0.00017622107554423767\n",
      "got 0.00017749866875998427\n",
      "\n",
      "Alice \t incorrect\n",
      "expected 0.00017622107554423767\n",
      "got 0.00017749866875998427\n",
      "\n",
      "Rabbit \t incorrect\n",
      "expected 0.00017622107554423767\n",
      "got 0.00017749866875998427\n",
      "\n",
      "Are dfs same? False\n"
     ]
    }
   ],
   "source": [
    "## print prob of words\n",
    "expected_probs = {'rabbit': 0.00010176146615934851,\n",
    "                  'it': 0.0002755005547240899,\n",
    "                  '[UNK]': 0.00017622107554423767,\n",
    "                  'Alice': 0.00017622107554423767,\n",
    "                  'Rabbit': 0.00017622107554423767,\n",
    "                 }\n",
    "\n",
    "for word in expected_probs:\n",
    "    if sample_model.get_prob(word) != expected_probs[word]:\n",
    "        print(word, '\\t incorrect')\n",
    "        print('expected', expected_probs[word])\n",
    "        print('got', sample_model.get_prob(word))\n",
    "        print()\n",
    "        \n",
    "\n",
    "## Create paths and then load it\n",
    "sample_model.evaluate(datafpath= 'data/sample-alice.txt',\n",
    "                      predfpath = 'predictions/my_sample_preds_alice.tsv')\n",
    "\n",
    "correct_df = pd.read_csv('predictions/sample_preds_alice.tsv', sep='\\t')\n",
    "my_df = pd.read_csv('predictions/my_sample_preds_alice.tsv', sep='\\t')\n",
    "\n",
    "## Does element wise comparison\n",
    "print('Are dfs same?', correct_df.equals(my_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5100a57-2064-4b98-a324-a170c0f7a588",
   "metadata": {},
   "source": [
    "## Part 2: Implement building blocks of a Naive Bayesian Classifier\n",
    "\n",
    "Implement the building blocks for a Bayesian classifier. Here is a function that might be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b6f41-3deb-439d-a3a4-fb2700893aab",
   "metadata": {},
   "source": [
    "### Part 2.1: Describe your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f20b5",
   "metadata": {},
   "source": [
    "p(class | text) ~ p(text | class) * p(class)\n",
    "\n",
    "How is unigram model related to likelihood?  \n",
    "p(text | class) is probability of the word from the unigram model trained on the class text.\n",
    "\n",
    "We can get p(text|class) by useing the unigram model trained on the class text, then we calculate p(class) using the 1/number of class. When we multiply the `likelihood` and the `prior`, we get the probability of the class given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b94e34-9d46-4f7f-9355-80d8e547eec7",
   "metadata": {},
   "source": [
    "### Part 2.2 Implement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c03a902-94c0-494e-958b-0cdd34350c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def summarize(fname:str, aggregrate_type:str, aggregrate_col:str, groupby_cols:list, delimiter='\\t'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fname: fpath to tsv/ csv file\n",
    "        aggregate_type: mean or sum\n",
    "\n",
    "        aggrefate_col: the column with values you want to aggregate over\n",
    "\n",
    "        groupby_cols: the columns with the groups. \n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe with as many rows as unique group combinations. The values of rows in each group is either summed together or averaged depending on the aggregate_type. \n",
    "\n",
    "    \"\"\"\n",
    "    dat = pd.read_csv(fname, sep=delimiter)\n",
    "\n",
    "    summ = dat.groupby(groupby_cols).agg({aggregrate_col: aggregrate_type}).reset_index()\n",
    "\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689d474-0107-4f15-954a-3f726fc06d78",
   "metadata": {},
   "source": [
    "#### **Calculating the likelihood**\n",
    "\n",
    "Start by calculating the likelihood of some text given models trained on text from different classes --- i.e., $P(text \\mid model=class1)$, $P(text \\mid model=class2)$, etc\n",
    "\n",
    "*Hint: Think about why `summarize` function provided is useful* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79577b49-c040-4df9-83c9-dc23c2a165b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(models_dict:dict, eval_fpath, class_label):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        models_dict: keys are classes and values are the models trained on the classes\n",
    "        eval_fpath: the file models should be evaluated on\n",
    "        class_label: the correct class label for sequences in the file \n",
    "\n",
    "    Returns:\n",
    "        A Dataframe with the following columns: \n",
    "            sentid: id of the sentence\n",
    "            model: the model being used to generate the likelihood\n",
    "            likelihood: the sum of log probability across all the words in the sequence\n",
    "            target_class: same as class_label\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    text = models_dict[class_label].preprocess([eval_fpath])\n",
    "    rows = []\n",
    "    for sentid in range(len(text)):\n",
    "        for model, um in models_dict.items():\n",
    "            likelihood = 0\n",
    "            sent = text[sentid]\n",
    "            for wordpos in range(len(sent)):\n",
    "                word = sent[wordpos]\n",
    "                prob = um.get_prob(word)\n",
    "                surp = -math.log2(prob)\n",
    "                likelihood += surp\n",
    "            rows.append([sentid, model, likelihood, class_label])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"sentid\", \"model\", \"likelihood\", \"class_label\"])\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9474e0-d9a6-42d7-a2b3-9386be26d280",
   "metadata": {},
   "source": [
    "Once you've implemented this function, verify that your output matches the expected output below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d58a3705-d506-400b-a186-e10e9e382085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentid</th>\n",
       "      <th>model</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>alice</td>\n",
       "      <td>180.358556</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>190.347402</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>alice</td>\n",
       "      <td>219.974713</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>235.950827</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>alice</td>\n",
       "      <td>195.032149</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>207.434474</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>alice</td>\n",
       "      <td>224.723169</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>236.211866</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>alice</td>\n",
       "      <td>142.610761</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>151.693819</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>alice</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>alice</td>\n",
       "      <td>205.949280</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>224.167940</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>alice</td>\n",
       "      <td>209.154067</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>223.249012</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>alice</td>\n",
       "      <td>224.498163</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>230.879937</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>alice</td>\n",
       "      <td>232.602981</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>246.410960</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>alice</td>\n",
       "      <td>165.626445</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>178.578879</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11</td>\n",
       "      <td>alice</td>\n",
       "      <td>83.654915</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>81.936670</td>\n",
       "      <td>alice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentid     model  likelihood class_label\n",
       "0        0     alice  180.358556       alice\n",
       "1        0  sherlock  190.347402       alice\n",
       "2        1     alice  219.974713       alice\n",
       "3        1  sherlock  235.950827       alice\n",
       "4        2     alice  195.032149       alice\n",
       "5        2  sherlock  207.434474       alice\n",
       "6        3     alice  224.723169       alice\n",
       "7        3  sherlock  236.211866       alice\n",
       "8        4     alice  142.610761       alice\n",
       "9        4  sherlock  151.693819       alice\n",
       "10       5     alice    0.000000       alice\n",
       "11       5  sherlock    0.000000       alice\n",
       "12       6     alice  205.949280       alice\n",
       "13       6  sherlock  224.167940       alice\n",
       "14       7     alice  209.154067       alice\n",
       "15       7  sherlock  223.249012       alice\n",
       "16       8     alice  224.498163       alice\n",
       "17       8  sherlock  230.879937       alice\n",
       "18       9     alice  232.602981       alice\n",
       "19       9  sherlock  246.410960       alice\n",
       "20      10     alice  165.626445       alice\n",
       "21      10  sherlock  178.578879       alice\n",
       "22      11     alice   83.654915       alice\n",
       "23      11  sherlock   81.936670       alice"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing proportion of matched values across correct_df and my_df\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (24,) (22,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#using this instead of equal because of floating point imprecision\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mPrinting proportion of matched values across correct_df and my_df\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mlikelihood\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlikelihood\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlikelihood\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.sum()/\u001b[38;5;28mlen\u001b[39m(my_df)) \n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33msentid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtarget_class\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(col, (my_df[col] == correct_df[col]).sum()/\u001b[38;5;28mlen\u001b[39m(my_df))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cosc426-projects/cosc426/.venv/lib/python3.13/site-packages/numpy/_core/numeric.py:2496\u001b[39m, in \u001b[36misclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2492\u001b[39m         \u001b[38;5;28mprint\u001b[39m(err_msg)\n\u001b[32m   2494\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2496\u001b[39m     result = (less_equal(\u001b[38;5;28mabs\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m), atol + rtol * \u001b[38;5;28mabs\u001b[39m(y))\n\u001b[32m   2497\u001b[39m               & isfinite(y)\n\u001b[32m   2498\u001b[39m               | (x == y))\n\u001b[32m   2499\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m equal_nan:\n\u001b[32m   2500\u001b[39m         result |= isnan(x) & isnan(y)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (24,) (22,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_models = {\n",
    "    'alice': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           ),\n",
    "    'sherlock': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-sherlock.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )\n",
    "}\n",
    "\n",
    "my_df = get_likelihood(sample_models, 'data/sample-lookingglass.txt', 'alice').reset_index(drop=True)\n",
    "correct_df = pd.read_csv('predictions/sample-likelihood.tsv', sep='\\t').reset_index(drop=True)\n",
    "\n",
    "#using this instead of equal because of floating point imprecision\n",
    "print('Printing proportion of matched values across correct_df and my_df\\n')\n",
    "print('likelihood', np.isclose(my_df['likelihood'], correct_df['likelihood']).sum()/len(my_df)) \n",
    "for col in ['sentid', 'model', 'target_class']:\n",
    "    print(col, (my_df[col] == correct_df[col]).sum()/len(my_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21711e91-0b7f-449e-a7d8-dde7227f8f84",
   "metadata": {},
   "source": [
    "#### Calculating the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09daf921-49e0-4e3f-8ff3-8d0b8307d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: dictionary where keys are the classes, and values are filepaths to the class specific data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prior probability for each class, which is the number of words in the class divided by the total number of words across all classes. \n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da052a32-3251-4fa5-8290-4e166b20fedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sherlock': 0.4423076923076923, 'alice': 0.5576923076923077}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict = {'sherlock': 'data/sample-sherlock.txt',\n",
    "            'alice': 'data/sample-alice.txt'}\n",
    "\n",
    "correct_prior = {'sherlock': 0.4423076923076923, 'alice': 0.5576923076923077}\n",
    "get_prior(dat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a096-f34d-4eaf-bdd5-5168161d0253",
   "metadata": {},
   "source": [
    "#### Compute posterior\n",
    "\n",
    "*Hint: Think about why `summarize` function provided is useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccfa1a27-5fde-416f-8b86-2c4eb18edefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(models_dict, eval_fpath, class_label, prior_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Dictionary where keys are classes and values are the models trained on the classes\n",
    "\n",
    "        eval_fpath: the file models should be evaluated on. \n",
    "\n",
    "        class_label: the label of the file that models are evaluated on\n",
    "\n",
    "        prior_dict: prior probabilities of classes\n",
    "\n",
    "    Returns:\n",
    "        A Dataframe with the following columns: sentid, model, likelihood, class. \n",
    "\n",
    "    If you set eval_fpath to sample_reviews_test_positive.txt, you should get a dataframe that looks like this. (Its ok if you end up having additional columns)\n",
    "\n",
    "   sentid model     likelihood   class        prior      posterior\n",
    "       0  positive  -100.975898  positive    -0.736966   -101.712864\n",
    "       1  positive  -100.941133  positive    -0.736966   -101.678099\n",
    "       0  negative  -101.938780  positive    -1.321928   -103.260708\n",
    "       1  negative  -101.938780  positive    -1.321928   -103.260708\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3072a5c3-f810-401c-8fa1-2450a2983d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior 1.0\n"
     ]
    }
   ],
   "source": [
    "my_df = get_posterior(sample_models, \n",
    "                    'data/sample-lookingglass.txt',\n",
    "                    'alice',\n",
    "                     get_prior(dat_dict)).reset_index(drop=True)\n",
    "\n",
    "correct_df = pd.read_csv('predictions/sample-posterior.tsv', sep='\\t').reset_index(drop=True)\n",
    "print('posterior', np.isclose(my_df['posterior'], correct_df['posterior']).sum()/len(my_df)) \n",
    "\n",
    "\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042209e7-e0b5-48f0-938e-9fa8ad00573a",
   "metadata": {},
   "source": [
    "#### Implement classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7c6cd5e-850f-4235-8c10-284d4040d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(posterior):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Dataframe with posterior probabilities\n",
    "\n",
    "    Returns: \n",
    "        Dataframe where each sentence id is associated with a prediction. \n",
    "    \"\"\"\n",
    "\n",
    "    # converts the data from long to wide\n",
    "    classes = posterior['model'].unique()\n",
    "    wide_df = posterior.pivot(index=['sentid', 'target_class'],\n",
    "                              columns=['model'],\n",
    "                              values='posterior').reset_index()    \n",
    "    #Finish the rest of the function \n",
    "\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0fab057-a729-4dad-b951-079b2295cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentid', 'target_class', 'alice', 'sherlock'], dtype='object', name='model')\n",
      "pred 1.0\n"
     ]
    }
   ],
   "source": [
    "posterior = get_posterior(sample_models, \n",
    "            'data/sample-lookingglass.txt',\n",
    "            'alice',\n",
    "             get_prior(dat_dict)).reset_index(drop=True)\n",
    "my_df = classify(posterior).reset_index(drop=True)\n",
    "\n",
    "correct_df = pd.read_csv('predictions/sample-classify.tsv', sep='\\t').reset_index(drop=True)\n",
    "print('pred', (my_df['pred']==correct_df['pred']).sum()/len(my_df)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d7d31-6c44-4ec8-a51f-4ceb5adf8e96",
   "metadata": {},
   "source": [
    "#### Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9af4294d-9418-41da-8386-8b2e26d1be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze(models_dict, eval_dict, prior_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        models_dict: keys are classes, values are models trained on data from the class. \n",
    "\n",
    "        eval_dict: keys are classes, values are fpaths to evaluation data where the correct label is the class associated with the key\n",
    "\n",
    "        prior_dict: keys are classes, values are prior probabilties of the classes. \n",
    "\n",
    "    Returns:\n",
    "        Float which is the accuracy of the predictions across all classes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df7d2694-ba6e-4476-92f4-e68bb294f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentid', 'target_class', 'alice', 'sherlock'], dtype='object', name='model')\n",
      "Index(['sentid', 'target_class', 'alice', 'sherlock'], dtype='object', name='model')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.473684210526315"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_models = {\n",
    "    'alice': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-alice.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           ),\n",
    "    'sherlock': UnigramModel(tokenize = util.nltk_tokenize,\n",
    "                            tokenizer_kwargs = {},\n",
    "                            vocab = util.get_vocab('data/glove_vocab.txt'),\n",
    "                            unk_token = '[UNK]',\n",
    "                            train_paths = ['data/sample-sherlock.txt'],\n",
    "                            smooth = 'add-0.1',\n",
    "                            lower = True\n",
    "                           )\n",
    "}\n",
    "\n",
    "## for simplicity making eval the same as train\n",
    "sample_eval = {\n",
    "    'alice': ['data/sample-alice.txt'],\n",
    "    'sherlock': ['data/sample-sherlock.txt']\n",
    "}\n",
    "\n",
    "sample_prior = get_prior({'sherlock': ['data/sample-sherlock.txt'],\n",
    "                          'alice': ['data/sample-alice.txt']})\n",
    "\n",
    "analyze(sample_models, sample_eval, sample_prior)\n",
    "\n",
    "target_acc = 39.473684210526315\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8cc45-0f9e-45ab-8ff0-bfb2d7a10f92",
   "metadata": {},
   "source": [
    "## Part 3: Build Naive Bayesian Sentiment Classifier\n",
    "Add as many code and markdown chunks as is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3a0e5-504c-438a-8228-af42cb09007b",
   "metadata": {},
   "source": [
    "## Part 4 (optional): Build Bigram Bayesian Sentiment Classifier\n",
    "Add as many code and markdown chunks as is helpful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
