{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2a2a61-9294-4606-95fd-2f946d44d95b",
   "metadata": {},
   "source": [
    "# Lab 7: Contextual Bag of Words with Pytorch\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab7.md`. Make sure to include in this notebook all the tests and experiments you run. Make sure to also cite any external resources you use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7883722-a497-47fc-9558-2989ad2fd1ed",
   "metadata": {},
   "source": [
    "## Part 1: Familiarize yourself with the different components of the CBOW model\n",
    "\n",
    "Add as many code chunks and markdown chunks as required to answer the questions in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "991157ba-9c0c-4f13-92ef-9227ebe58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import CBOW\n",
    "\n",
    "# For colorcoding\n",
    "RED = \"\\033[31m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c6231-c18d-40c1-afe2-8cd828fe512d",
   "metadata": {},
   "source": [
    "### Part 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df030a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = CBOW.CBOW_Dataset(\"./data/sample-alice.txt\", \"./data/sample_vocab.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c10de8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training examples: 336\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num training examples: {len(data4.X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4f488c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = CBOW.CBOW_Dataset(\"./data/sample-alice.txt\", \"./data/sample_vocab.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af6040b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training examples: 336\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num training examples: {len(data2.X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51fa41",
   "metadata": {},
   "source": [
    "1. Create a `CBOW_Dataset` object with `sample_alice.txt`, `sample_vocab.txt`, and a window size of 4. How many training examples does the dataset have? (*Hint: Remember you can access the variables in the object's init*) \n",
    "\n",
    "    There are 336 training examples.\n",
    "\n",
    "2. Create a `CBOW_Dataset` object with `sample_alice.txt`, `sample_vocab.txt`, and a window size of 2. How many training examples does the dataset have?\n",
    "\n",
    "    There are 336 training examples.\n",
    "\n",
    "3. Does window size affect the number of training examples, why or why not? \n",
    "\n",
    "    Since we are using padding, both of the training sets have the same number of examples, which is the number of words in the text.\n",
    "\n",
    "4. What does one training example look like? What is the sequence of steps to go from text in the form of a string to the final format and data types of the training example? \n",
    "\n",
    "    A training example looks like an embedding, of widow size*2. We want to use a *2 since we want to account for every word, hence we need to add padding so that the window accounts for every word. We frist divide the text into sentences, and further divide the sentences into tokens. Tokens are then converted into integer values using the vocab:id mapping. We chunk these values into pairs in the format of n words:n+1th word, where n is the window size. \n",
    "\n",
    "5. Is this final dataset case-sensitive (i.e., does it treat lower and upper case differently)? If it is, how can you change it to not be case-sensitive and vice versa? \n",
    "\n",
    "    The final dataset is not case sensitive. To make it case sensitive, we can remove `.lower()` funciton in the preprocessing, and treat capitalized words as diffrent tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce27bc-4a58-4d4a-bfb2-a5d142b838e5",
   "metadata": {},
   "source": [
    "### Part 1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76992390",
   "metadata": {},
   "source": [
    "1. In the `__init__` part of the `CBOW_Model`, you are initializing a `torch.nn.Embedding` layer. Conceptually, what does this layer do? What are the input and output dimensions of this layer? Why does this make sense?\n",
    "\n",
    "    Conceptually, the torch.nn.Embedding layer is weights that convert input into output, and the number of weights we have is the size of the input * the size of the output. The dimensions of the input is the size of the vocab and the output is nEmbed. This makes sense becuase we want the embedding to be denser then the vocabasize.\n",
    "\n",
    "2. You are also initializing a `torch.nn.Linear` layer. Conceptually, what does this layer do? What are the input and output dimensions of this layer? Why does this make sense? \n",
    "\n",
    "    Conceptually, the linear layer is the weights that convert the output vecotr of the Embedding layer to the vector representation of a word. The input dimension is the same as `nEmbed` and the output dimension is the vocabsize. This makes sense because in the end we want the result to be in the size of the vocab. \n",
    "\n",
    "3. Describe in your own words what you think is happening in the `forward` function. \n",
    "\n",
    "    The `foward` function multiplies the embedding weights to the input and calcualtes the average.\n",
    "\n",
    "4. Describe in your own words what you think is happening in the `loss` function.\n",
    "\n",
    "    The `loss` fucntion calculates the crossenthropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8e1a9-65bb-4260-ba04-74ece6d135f2",
   "metadata": {},
   "source": [
    "### Part 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a858f54",
   "metadata": {},
   "source": [
    "1. When you create a `CBOW_Trainer` object, you also have to pass in the following parameters. Explain how these paramters are used during training? \n",
    "\n",
    "   - num_epochs: how many times we want to repeat the training process.\n",
    "   - lr: the amount change we want to make in each epoch\n",
    "   - batch_size: the amount of data we want to pass at each forward. \n",
    "   - train_data: the data we use to train the model\n",
    "   - val_data: the data we use to minimize the loss\n",
    "   - device: the device we want to use(cpu, cuda etc.)\n",
    "\n",
    "2. We are loading training data using `torch.utils.data.DataLoader`. What are the parameters for this function? What is the format in which this function returns the data? \n",
    "\n",
    "    - train_data: dataset from which to load the data.\n",
    "    - batch_size: how many samples per batch to load\n",
    "    - shuffle: set to True to have the data reshuffled at every epoch (default: False).\n",
    "\n",
    "    It returns an iterable list of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee223ea2",
   "metadata": {},
   "source": [
    "3. The following line calls the `forward` function of the model and saves the output. \n",
    "    ```\n",
    "    y_pred = model(X)\n",
    "    ```\n",
    "    Describe what you think each of the following lines are doing. \n",
    "    \n",
    "   * `X,y_target = X.to(self.device), y_target.to(self.device)`: Moves training data from the memory to GPU. \n",
    "   * `loss = model.loss(y_pred, y_target)`: computes the CrossEntropyLoss loss.\n",
    "   * `optimizer.zero_grad()`: Clearing the gradient for the next update. \n",
    "   * `loss.backward()`: Adjusts the weight to optimize the model(to minimize the loss of the model). Calculating the gradient. \n",
    "   * `optimizer.step()`: Updates the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731643c-06fa-4a12-9719-9e454d2b4d31",
   "metadata": {},
   "source": [
    "### Part 1.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33b20c",
   "metadata": {},
   "source": [
    "1. Describe what `compute_loss` and `get_preds` do. Which of these two functions is closest to the evaluate mode in `NLPScholar`?\n",
    "\n",
    "    First the `compute_loss` function loads the training data to the selected device(e.g., CPU, GPU), then it predicts y given X by using the model. After the prediction, the function calculates the loss of the predicted result by comparing it to the expected result. This process gets reapted for all the examples in the data to calculate the total loss which then gets divied by the number of examples to get the average loss. \n",
    "\n",
    "    Instead of calculating the loss of the predicted values, the `get_preds` simply finds and returns the words that the model assigned the highest value to for each of the trainning examples. \n",
    "\n",
    "    I would say the latter function, `get_preds` is closer to the evaluate mode in `NLPScholar` becuase NLPScholar also focuses on the accuracy of each prediction rather then the loss. \n",
    "\n",
    "2. Why do these functions have `@torch.nograd`?\n",
    "\n",
    "    Gradients are used in a process of training a model. However, since the functions are only used in the context of evaluating the models, gradients are not used and thus there is no need to calculate them.\n",
    "\n",
    "3. Say you have a `CBOW_Model` object `model`, and a `CBOW_Dataset` object called `test_data`. How will you use this class to calculate the loss of `model` on `test_data`? \n",
    "\n",
    "    ```python\n",
    "    evaluator = CBOW_Evaluator(test_data, batch_size, device)\n",
    "    evaluator.compute_loss(model)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b3718-def4-4d30-95bb-f8fa6844c52d",
   "metadata": {},
   "source": [
    "## Part 2: Train and evaluate a CBOW model on toy data, and explore the word embeddings\n",
    "\n",
    "Add as many code chunks and markdown chunks as required to answer the questions in this part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae946ac3-2e82-4fa6-81c5-bc2b50928095",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"cuda\": True, \"mps\": True}  # cuda ==> mps ==> cpu\n",
    "\n",
    "\n",
    "def get_device(args: dict):\n",
    "    # using get. removed not\n",
    "    use_cuda = args.get(\"cuda\", False) and torch.cuda.is_available()\n",
    "    use_mps = args.get(\"mps\", False) and torch.backends.mps.is_available()\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_device(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4e93-be5d-451c-a105-2af22a8bc934",
   "metadata": {},
   "source": [
    "### Initialize model and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "142d0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CBOW.CBOW_Dataset(\n",
    "    fname=\"./data/sample-alice.txt\",\n",
    "    vocab_fname=\"./data/sample_vocab.txt\",\n",
    "    window_size=4,\n",
    ")\n",
    "\n",
    "val_data = CBOW.CBOW_Dataset(\n",
    "    fname=\"./data/sample-lookingglass.txt\",\n",
    "    vocab_fname=\"./data/sample_vocab.txt\",\n",
    "    window_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41d58168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW.CBOW_Model(50, train_data.vocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee894700-edad-4ecd-b7e4-81aff8bd816d",
   "metadata": {},
   "source": [
    "### Evaluate randomly initialized model\n",
    "\n",
    "* Report loss and accuracy on the training data\n",
    "* Report cosine similarity between the 4 word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ab2debf-1778-42d0-a8a9-20ccdb61caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = CBOW.CBOW_Evaluator(\n",
    "    test_data=val_data,\n",
    "    batch_size=8,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ea829c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # sending the model to CUDA\n",
    "\n",
    "# loss\n",
    "loss_rand = ev.compute_loss(model)\n",
    "\n",
    "# acc\n",
    "gold, pred_rand = ev.get_preds(model)\n",
    "acc_rand = (torch.stack(gold) == torch.stack(pred_rand)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b340312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mRandomly Initialzied Model\u001b[0m\n",
      "----------------------------------------\n",
      "Loss: 5.05\n",
      "Accuracy: 0.54%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Randomly Initialzied Model{END}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Loss: {loss_rand:.2f}\")\n",
    "print(f\"Accuracy: {acc_rand*100:.2f}%\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa98e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mCosine Similarities\u001b[0m\n",
      "----------------------------------------\n",
      "1. think-thought: \u001b[31m0.12\u001b[0m\n",
      "2. think-tired: \u001b[31m-0.05\u001b[0m\n",
      "3. sleepy-thought: \u001b[31m0.04\u001b[0m\n",
      "4. sleepy-tired: \u001b[31m0.03\u001b[0m\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "embedding_rand = model.embed.weight.data\n",
    "\n",
    "seqs = [\n",
    "    [\"think\", \"thought\"],\n",
    "    [\"think\", \"tired\"],\n",
    "    [\"sleepy\", \"thought\"],\n",
    "    [\"sleepy\", \"tired\"],\n",
    "]\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Cosine Similarities{END}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, seq in enumerate(seqs):\n",
    "    w1, w2 = train_data.encode(seq)\n",
    "\n",
    "    v1 = embedding_rand[w1]\n",
    "    v2 = embedding_rand[w2]\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(v1, v2, dim=0)\n",
    "\n",
    "    print(f\"{idx+1}. {seq[0]}-{seq[1]}: {RED}{similarity:.2f}{END}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e83258-eef1-4e51-a65d-8d8f16750fe7",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3f841bf-0bd4-4fb9-a360-5363a8bb9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CBOW.CBOW_Trainer(\n",
    "    num_epochs=1000,\n",
    "    lr=0.5,\n",
    "    batch_size=8,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "319d6307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Avg Train Loss: 4.78259\t Avg Val Loss: 5.44737\n",
      "Epoch 20:\t Avg Train Loss: 0.05395\t Avg Val Loss: 10.30407\n",
      "Epoch 40:\t Avg Train Loss: 0.02395\t Avg Val Loss: 10.69626\n",
      "Epoch 60:\t Avg Train Loss: 0.02867\t Avg Val Loss: 10.76187\n",
      "Epoch 80:\t Avg Train Loss: 0.01161\t Avg Val Loss: 11.02757\n",
      "Epoch 100:\t Avg Train Loss: 0.01075\t Avg Val Loss: 11.25019\n",
      "Epoch 120:\t Avg Train Loss: 0.01036\t Avg Val Loss: 11.51962\n",
      "Epoch 140:\t Avg Train Loss: 0.01007\t Avg Val Loss: 11.64922\n",
      "Epoch 160:\t Avg Train Loss: 0.01118\t Avg Val Loss: 11.75806\n",
      "Epoch 180:\t Avg Train Loss: 0.00971\t Avg Val Loss: 11.82172\n",
      "Epoch 200:\t Avg Train Loss: 0.00898\t Avg Val Loss: 11.90032\n",
      "Epoch 220:\t Avg Train Loss: 0.01032\t Avg Val Loss: 12.04747\n",
      "Epoch 240:\t Avg Train Loss: 0.01166\t Avg Val Loss: 12.10977\n",
      "Epoch 260:\t Avg Train Loss: 0.00688\t Avg Val Loss: 12.18542\n",
      "Epoch 280:\t Avg Train Loss: 0.00725\t Avg Val Loss: 12.24286\n",
      "Epoch 300:\t Avg Train Loss: 0.00855\t Avg Val Loss: 12.32814\n",
      "Epoch 320:\t Avg Train Loss: 0.00818\t Avg Val Loss: 12.44007\n",
      "Epoch 340:\t Avg Train Loss: 0.0089\t Avg Val Loss: 12.44665\n",
      "Epoch 360:\t Avg Train Loss: 0.00952\t Avg Val Loss: 12.5367\n",
      "Epoch 380:\t Avg Train Loss: 0.01322\t Avg Val Loss: 12.5856\n",
      "Epoch 400:\t Avg Train Loss: 0.01092\t Avg Val Loss: 12.61523\n",
      "Epoch 420:\t Avg Train Loss: 0.00808\t Avg Val Loss: 12.71845\n",
      "Epoch 440:\t Avg Train Loss: 0.01228\t Avg Val Loss: 12.72371\n",
      "Epoch 460:\t Avg Train Loss: 0.01337\t Avg Val Loss: 12.80022\n",
      "Epoch 480:\t Avg Train Loss: 0.00649\t Avg Val Loss: 12.90917\n",
      "Epoch 500:\t Avg Train Loss: 0.0079\t Avg Val Loss: 12.90759\n",
      "Epoch 520:\t Avg Train Loss: 0.00681\t Avg Val Loss: 12.90867\n",
      "Epoch 540:\t Avg Train Loss: 0.01073\t Avg Val Loss: 13.01291\n",
      "Epoch 560:\t Avg Train Loss: 0.011\t Avg Val Loss: 13.07024\n",
      "Epoch 580:\t Avg Train Loss: 0.00803\t Avg Val Loss: 13.08363\n",
      "Epoch 600:\t Avg Train Loss: 0.00644\t Avg Val Loss: 13.10565\n",
      "Epoch 620:\t Avg Train Loss: 0.00762\t Avg Val Loss: 13.13827\n",
      "Epoch 640:\t Avg Train Loss: 0.00877\t Avg Val Loss: 13.16933\n",
      "Epoch 660:\t Avg Train Loss: 0.01003\t Avg Val Loss: 13.21085\n",
      "Epoch 680:\t Avg Train Loss: 0.00861\t Avg Val Loss: 13.2318\n",
      "Epoch 700:\t Avg Train Loss: 0.00766\t Avg Val Loss: 13.32876\n",
      "Epoch 720:\t Avg Train Loss: 0.01082\t Avg Val Loss: 13.2911\n",
      "Epoch 740:\t Avg Train Loss: 0.00964\t Avg Val Loss: 13.34652\n",
      "Epoch 760:\t Avg Train Loss: 0.00961\t Avg Val Loss: 13.39794\n",
      "Epoch 780:\t Avg Train Loss: 0.00735\t Avg Val Loss: 13.3864\n",
      "Epoch 800:\t Avg Train Loss: 0.00711\t Avg Val Loss: 13.41696\n",
      "Epoch 820:\t Avg Train Loss: 0.0079\t Avg Val Loss: 13.46336\n",
      "Epoch 840:\t Avg Train Loss: 0.00744\t Avg Val Loss: 13.45145\n",
      "Epoch 860:\t Avg Train Loss: 0.00559\t Avg Val Loss: 13.5252\n",
      "Epoch 880:\t Avg Train Loss: 0.00861\t Avg Val Loss: 13.53984\n",
      "Epoch 900:\t Avg Train Loss: 0.00686\t Avg Val Loss: 13.5771\n",
      "Epoch 920:\t Avg Train Loss: 0.00902\t Avg Val Loss: 13.67217\n",
      "Epoch 940:\t Avg Train Loss: 0.00557\t Avg Val Loss: 13.63643\n",
      "Epoch 960:\t Avg Train Loss: 0.00772\t Avg Val Loss: 13.70764\n",
      "Epoch 980:\t Avg Train Loss: 0.00756\t Avg Val Loss: 13.65163\n",
      "Training done!\n",
      "Avg Train Loss: 0.00763\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model)\n",
    "\n",
    "# rand_model_path = \"./models/rand.pkl\"\n",
    "\n",
    "# if not os.path.exists(rand_model_path):\n",
    "#     trainer.train(model)\n",
    "#     with open(rand_model_path, 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "# with open(rand_model_path, \"rb\") as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc44d7-1d6a-48d1-aefb-941d3de44622",
   "metadata": {},
   "source": [
    "### Evaluate trained model\n",
    "\n",
    "* Report loss and accuracy on the training data\n",
    "* Report cosine similarity between the 4 word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54ffa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # sending the model to CUDA\n",
    "\n",
    "# loss\n",
    "loss_1000 = ev.compute_loss(model)\n",
    "\n",
    "# acc\n",
    "gold, pred_1000 = ev.get_preds(model)\n",
    "acc_1000 = (torch.stack(gold) == torch.stack(pred_1000)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "469c804b-3f28-4cd5-9438-8107d9fee6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mRandomly Initialzied Model\u001b[0m\n",
      "----------------------------------------\n",
      "Loss: 13.69\n",
      "Accuracy: 9.24%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Randomly Initialzied Model{END}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Loss: {loss_1000:.2f}\")\n",
    "print(f\"Accuracy: {acc_1000*100:.2f}%\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93352f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mCosine Similarities: 1000 epochs\u001b[0m\n",
      "----------------------------------------\n",
      "1. think-thought: \u001b[31m0.44\u001b[0m\n",
      "2. think-tired: \u001b[31m-0.16\u001b[0m\n",
      "3. sleepy-thought: \u001b[31m0.13\u001b[0m\n",
      "4. sleepy-tired: \u001b[31m0.09\u001b[0m\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "embedding_1000 = model.embed.weight.data\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Cosine Similarities: 1000 epochs{END}\")\n",
    "print(\"-\" * 40)\n",
    "for idx, seq in enumerate(seqs):\n",
    "    w1, w2 = train_data.encode(seq)\n",
    "\n",
    "    v1 = embedding_1000[w1]\n",
    "    v2 = embedding_1000[w2]\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(v1, v2, dim=0)\n",
    "\n",
    "    print(f\"{idx+1}. {seq[0]}-{seq[1]}: {RED}{similarity:.2f}{END}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48bc7a-401d-4287-af51-0200c625558c",
   "metadata": {},
   "source": [
    "### Discussion and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47dd85",
   "metadata": {},
   "source": [
    "7. Do you think that the model has learned the task? Do you think the model has learned useful embeddings?\n",
    "\n",
    "I think the model has learned the task, but only to a very limited degree. While the accuracy increased significantly from almost 0.54% to around 9%, 9% as an absolute measure is still quite low. It is interesting that the validation loss has doubled while the accuracy has increased. The model still seems to struggle with cosine similarity measures, as the \"sleepy-tired\" pair still has a very low cosine similarity. However, relative to the original embedding, this is a notable improvement since now some of the pairs(\"think-thought\", \"sleepy-thought\") have more sensible values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb69de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.vocabSize  # vocab size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39605b",
   "metadata": {},
   "source": [
    "8. Does it make sense to use embedding size of 300 for this toy data? Why or why not? \n",
    "\n",
    "In this case, the embedding size of 300 does not make sense since the vocab size(151) is much smaller than 300. If we use 300 as the embedding size, we are making the representation sparser, not denser. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931af9d8-ff75-4a19-91bb-f82867442f99",
   "metadata": {},
   "source": [
    "## Part 3: Explore the role of training data on the word embeddings that are learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2546f6d-e745-4881-82ff-2cf8fefcd67f",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b0acb",
   "metadata": {},
   "source": [
    "#### Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# coca_vocab_10k.txt was missing in the data folder\n",
    "vocab_fname = \"./data/coca_vocab_10k.txt\"\n",
    "\n",
    "# Source: https://www.eapfoundation.com/vocab/general/bnccoca/#listfreq\n",
    "df = pd.read_excel(\"./data/BNC_COCA_lists.xlsx\", sheet_name=\"Sheet1\")\n",
    "df = df.sort_values(by=\"Total frequency\", ascending=False)[:10_000]\n",
    "\n",
    "df[\"Headword \"].to_csv(vocab_fname, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81824f-7e62-442f-873b-211bdc5d142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_train_data = CBOW.CBOW_Dataset(\n",
    "    fname=\"./data/alice_in_wonderland.txt\",\n",
    "    vocab_fname=vocab_fname,\n",
    "    window_size=4,\n",
    ")\n",
    "\n",
    "aiw_model = CBOW.CBOW_Model(50, aiw_train_data.vocabSize)\n",
    "aiw_model = aiw_model.to(device)\n",
    "\n",
    "aiw_trainer = CBOW.CBOW_Trainer(\n",
    "    num_epochs=50,\n",
    "    lr=0.1,\n",
    "    batch_size=100,\n",
    "    train_data=aiw_train_data,\n",
    "    val_data=aiw_train_data,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3382aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Avg Train Loss: 3.83196\t Avg Val Loss: 3.35886\n",
      "Epoch 20:\t Avg Train Loss: 2.72914\t Avg Val Loss: 2.69802\n",
      "Epoch 40:\t Avg Train Loss: 2.54592\t Avg Val Loss: 2.50947\n",
      "Training done!\n",
      "Avg Train Loss: 2.48387\n"
     ]
    }
   ],
   "source": [
    "aiw_trainer.train(aiw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87ae231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_eval = CBOW.CBOW_Evaluator(\n",
    "    test_data=aiw_train_data,\n",
    "    batch_size=100,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1c4ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mAlice in Wonderland Model\u001b[0m\n",
      "----------------------------------------\n",
      "Loss: 2.46\n",
      "Accuracy: 49.82%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "loss_aiw = aiw_eval.compute_loss(aiw_model)\n",
    "\n",
    "# acc\n",
    "gold_aiw, pred_aiw = aiw_eval.get_preds(aiw_model)\n",
    "acc_aiw = (torch.stack(gold_aiw) == torch.stack(pred_aiw)).float().mean()\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Alice in Wonderland Model{END}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Loss: {loss_aiw:.2f}\")\n",
    "print(f\"Accuracy: {acc_aiw*100:.2f}%\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c096666",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a616a",
   "metadata": {},
   "source": [
    "#### Sherlock Homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca39d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_train_data = CBOW.CBOW_Dataset(\n",
    "    fname=\"./data/sherlock_holmes_short.txt\",\n",
    "    vocab_fname=vocab_fname,\n",
    "    window_size=4,\n",
    ")\n",
    "\n",
    "sh_model = CBOW.CBOW_Model(50, sh_train_data.vocabSize)\n",
    "sh_model = sh_model.to(device)\n",
    "\n",
    "sh_trainer = CBOW.CBOW_Trainer(\n",
    "    num_epochs=50,\n",
    "    lr=0.1,\n",
    "    batch_size=100,\n",
    "    train_data=sh_train_data,\n",
    "    val_data=sh_train_data,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "sh_eval = CBOW.CBOW_Evaluator(\n",
    "    test_data=sh_train_data,\n",
    "    batch_size=100,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9e64c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Avg Train Loss: 4.18429\t Avg Val Loss: 3.62921\n",
      "Epoch 20:\t Avg Train Loss: 3.00917\t Avg Val Loss: 2.97163\n",
      "Epoch 40:\t Avg Train Loss: 2.80457\t Avg Val Loss: 2.77392\n",
      "Training done!\n",
      "Avg Train Loss: 2.73094\n"
     ]
    }
   ],
   "source": [
    "sh_trainer.train(sh_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c773fbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[94mSherlock Holmes Model\u001b[0m\n",
      "----------------------------------------\n",
      "Loss: 2.71\n",
      "Accuracy: 45.78%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "loss_sh = sh_eval.compute_loss(sh_model)\n",
    "\n",
    "# acc\n",
    "gold_sh, pred_sh = sh_eval.get_preds(sh_model)\n",
    "acc_sh = (torch.stack(gold_sh) == torch.stack(pred_sh)).float().mean()\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"{BLUE}Sherlock Holmes Model{END}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Loss: {loss_sh:.2f}\")\n",
    "print(f\"Accuracy: {acc_sh*100:.2f}%\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a958d-6607-4480-a298-4d80a8ace087",
   "metadata": {},
   "source": [
    "### Come up with list of words\n",
    "Answer questions 2 and 3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = [\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"of\",\n",
    "    \"a\",\n",
    "    \"to\",\n",
    "    \"in\",\n",
    "    \"is\",\n",
    "    \"was\",\n",
    "    \"with\",\n",
    "    \"it\",\n",
    "    \"that\",\n",
    "    \"from\",\n",
    "]\n",
    "\n",
    "vars = [  # aiw / sh\n",
    "    \"mad\",  # hatter / angry\n",
    "    \"queen\",  # white|hearts / royalty\n",
    "    \"white\",  # queen / color\n",
    "    \"house\",  # rabbit / home\n",
    "    \"case\",  # box / murder\n",
    "    \"time\",  # rabbit / clock\n",
    "    \"king\",  # herats / royalty\n",
    "    \"curious\",  # Alice / case\n",
    "    \"mystery\",  # wonder / crime\n",
    "    \"off\",  # heads / something\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c287326",
   "metadata": {},
   "source": [
    "Since the words in the list `sims` are largely funtion as just grammatical words, they should have the same embeddigns across different texts. \n",
    "\n",
    "On the other hand, the words in vars list are context heavy words. For example, the word `mad` should have a similar embedding as the word `hatter` in the aiw model, but not in sh model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388b1f9-c3cc-4b7f-a47e-ca4e895837d4",
   "metadata": {},
   "source": [
    "### Test your hypotheses\n",
    "\n",
    "Answer question 4 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6e709-1c0e-418c-aa7e-9b821573edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cst(vs1: list[torch.Tensor], vs2: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Given two lists of vectors, returns a list of their cosine similarities.\n",
    "\n",
    "    Args:\n",
    "        vs1 (list[torch.Tensor]): list of vectors\n",
    "        vs2 (list[torch.Tensor]): list of vectors\n",
    "\n",
    "    Returns:\n",
    "        list[torch.Tensor]: list of similarities\n",
    "    \"\"\"\n",
    "    if len(vs1) != len(vs2):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(vs1)):\n",
    "        similarity = torch.nn.functional.cosine_similarity(vs1[i], vs2[i], dim=0)\n",
    "        result.append(similarity)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d3db54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiw_encoded_sims = aiw_train_data.encode(sims)\n",
    "aiw_encoded_vars = aiw_train_data.encode(vars)\n",
    "aiw_sims = []\n",
    "aiw_vars = []\n",
    "\n",
    "for sim in aiw_encoded_sims:\n",
    "    aiw_sims.append(aiw_model.embed.weight.data[sim])\n",
    "for var in aiw_encoded_vars:\n",
    "    aiw_vars.append(aiw_model.embed.weight.data[var])\n",
    "\n",
    "sh_encoded_sims = sh_train_data.encode(sims)\n",
    "sh_encoded_vars = sh_train_data.encode(vars)\n",
    "sh_sims = []\n",
    "sh_vars = []\n",
    "\n",
    "for sim in sh_encoded_sims:\n",
    "    sh_sims.append(sh_model.embed.weight.data[sim])\n",
    "for var in sh_encoded_vars:\n",
    "    sh_vars.append(sh_model.embed.weight.data[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9fc16699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_cst = cst(aiw_sims, sh_sims)\n",
    "vars_cst = cst(aiw_vars, sh_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c32e6d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. the: 0.00\n",
      "2. and: -0.09\n",
      "3. of: -0.36\n",
      "4. a: 0.01\n",
      "5. to: -0.01\n",
      "6. in: -0.19\n",
      "7. is: 0.03\n",
      "8. was: 0.03\n",
      "9. with: 0.02\n",
      "10. it: -0.01\n",
      "11. that: 0.02\n",
      "12. from: -0.03\n",
      "----------------------------------------\n",
      "Ave. Cosine Similarity: -0.05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sims_cst)):\n",
    "    print(f\"{i+1}. {sims[i]}: {sims_cst[i]:.2f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Ave. Cosine Similarity: {sum(sims_cst)/len(sims_cst):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3d29d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. mad: 0.10\n",
      "2. queen: 0.09\n",
      "3. white: -0.02\n",
      "4. house: 0.37\n",
      "5. case: -0.05\n",
      "6. time: -0.21\n",
      "7. king: -0.13\n",
      "8. curious: 0.07\n",
      "9. mystery: -0.07\n",
      "10. off: 0.06\n",
      "----------------------------------------\n",
      "Ave. Cosine Similarity: 0.02\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(vars_cst)):\n",
    "    print(f\"{i+1}. {vars[i]}: {vars_cst[i]:.2f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Ave. Cosine Similarity: {sum(vars_cst)/len(vars_cst):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f672ad",
   "metadata": {},
   "source": [
    "The fact that the \"similar\" words list had a lower average cosine similarity (-0.05) than the \"varied\" words list (0.02) seems to indicate that my hypothesis was wrong. However, I suspect that this result stems from the models being undertrained. Both average similarity scores are, for all practical purposes, effectively zero, which suggests the word embeddings have not learned meaningful representations and are still close to their random initialization. The training corpora (Alice in Wonderland and Sherlock Holmes) are far too small—containing only thousands of words—to adequately train vectors for a 10,000-word vocabulary. In a high-dimensional space, the similarity between any two random vectors is expected to be near 0. Therefore, the minor difference between -0.05 and 0.02 is not a meaningful signal of learned semantic relationships, but rather a product of statistical noise and data sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4810335-528e-439d-861b-dbe4adf0ee1d",
   "metadata": {},
   "source": [
    "## Part 4 (Optional): Explore the role of other factors on the word embeddings that are learned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
